2022-07-29 18:20:52.576191: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
  from pandas import Int64Index as NumericIndex
/home/jonas2/DeepLabCut/deeplabcut/__init__.py:81: UserWarning: 
        As PyTorch is not installed, unsupervised identity learning will not be available.
        Please run `pip install torch`, or ignore this warning.
        
  warnings.warn(
Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15]],
 'all_joints_names': ['t3L',
                      'wstL',
                      't5L',
                      'elbL',
                      'shdL',
                      'ankL',
                      'nl',
                      'str',
                      'lmb',
                      'shdR',
                      'ankR',
                      'elbR',
                      'wstR',
                      't5R',
                      't3R',
                      'tail'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 8,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC80shuffle1.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'fliplr': False,
 'gaussian_blur': False,
 'gaussian_blur_params': {'sigma': [0.5, 4.0]},
 'global_scale': 0.8,
 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_80shuffle1.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]],
 'net_type': 'resnet_101',
 'num_joints': 16,
 'optimizer': 'adam',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset80shuffle1/train/snapshot',
 'stride': 8.0,
 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]],
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2022-07-29 18:21:01.154181: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-29 18:21:01.624429: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-07-29 18:21:01.624508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22344 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:67:00.0, compute capability: 8.6
2022-07-29 18:21:02.593225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22344 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:67:00.0, compute capability: 8.6
2022-07-29 18:21:04.615825: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2022-07-29 18:21:20.609131: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401
/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.
  warnings.warn('`layer.apply` is deprecated and '
iteration: 1000 loss: 0.0263 lr: 0.0001
iteration: 2000 loss: 0.0134 lr: 0.0001
iteration: 3000 loss: 0.0112 lr: 0.0001
iteration: 4000 loss: 0.0100 lr: 0.0001
iteration: 5000 loss: 0.0090 lr: 0.0001
iteration: 6000 loss: 0.0086 lr: 0.0001
iteration: 7000 loss: 0.0079 lr: 0.0001
iteration: 8000 loss: 0.0072 lr: 5e-05
iteration: 9000 loss: 0.0061 lr: 5e-05
iteration: 10000 loss: 0.0061 lr: 5e-05
iteration: 11000 loss: 0.0058 lr: 5e-05
iteration: 12000 loss: 0.0057 lr: 5e-05
iteration: 13000 loss: 0.0048 lr: 1e-05
iteration: 14000 loss: 0.0047 lr: 1e-05
iteration: 15000 loss: 0.0045 lr: 1e-05
iteration: 16000 loss: 0.0043 lr: 1e-05
iteration: 17000 loss: 0.0043 lr: 1e-05
iteration: 18000 loss: 0.0043 lr: 1e-05
iteration: 19000 loss: 0.0042 lr: 1e-05
iteration: 20000 loss: 0.0042 lr: 1e-05
iteration: 21000 loss: 0.0041 lr: 1e-05
iteration: 22000 loss: 0.0041 lr: 1e-05
iteration: 23000 loss: 0.0040 lr: 1e-05
iteration: 24000 loss: 0.0040 lr: 1e-05
iteration: 25000 loss: 0.0039 lr: 1e-05
iteration: 26000 loss: 0.0039 lr: 1e-05
iteration: 27000 loss: 0.0040 lr: 1e-05
iteration: 28000 loss: 0.0039 lr: 1e-05
iteration: 29000 loss: 0.0039 lr: 1e-05
iteration: 30000 loss: 0.0038 lr: 1e-05
iteration: 31000 loss: 0.0037 lr: 1e-05
iteration: 32000 loss: 0.0037 lr: 1e-05
iteration: 33000 loss: 0.0037 lr: 1e-05
iteration: 34000 loss: 0.0037 lr: 1e-05
iteration: 35000 loss: 0.0037 lr: 1e-05
iteration: 36000 loss: 0.0037 lr: 1e-05
iteration: 37000 loss: 0.0036 lr: 1e-05
iteration: 38000 loss: 0.0036 lr: 1e-05
iteration: 39000 loss: 0.0036 lr: 1e-05
iteration: 40000 loss: 0.0035 lr: 1e-05
iteration: 41000 loss: 0.0035 lr: 1e-05
iteration: 42000 loss: 0.0036 lr: 1e-05
iteration: 43000 loss: 0.0034 lr: 1e-05
iteration: 44000 loss: 0.0035 lr: 1e-05
iteration: 45000 loss: 0.0034 lr: 1e-05
iteration: 46000 loss: 0.0034 lr: 1e-05
iteration: 47000 loss: 0.0034 lr: 1e-05
iteration: 48000 loss: 0.0034 lr: 1e-05
iteration: 49000 loss: 0.0033 lr: 1e-05
iteration: 50000 loss: 0.0033 lr: 1e-05
iteration: 51000 loss: 0.0033 lr: 1e-05
iteration: 52000 loss: 0.0034 lr: 1e-05
iteration: 53000 loss: 0.0033 lr: 1e-05
iteration: 54000 loss: 0.0033 lr: 1e-05
iteration: 55000 loss: 0.0033 lr: 1e-05
iteration: 56000 loss: 0.0033 lr: 1e-05
iteration: 57000 loss: 0.0032 lr: 1e-05
iteration: 58000 loss: 0.0032 lr: 1e-05
iteration: 59000 loss: 0.0033 lr: 1e-05
iteration: 60000 loss: 0.0033 lr: 1e-05
iteration: 61000 loss: 0.0032 lr: 1e-05
iteration: 62000 loss: 0.0031 lr: 1e-05
iteration: 63000 loss: 0.0032 lr: 1e-05
iteration: 64000 loss: 0.0031 lr: 1e-05
iteration: 65000 loss: 0.0031 lr: 1e-05
iteration: 66000 loss: 0.0031 lr: 1e-05
iteration: 67000 loss: 0.0030 lr: 1e-05
iteration: 68000 loss: 0.0030 lr: 1e-05
iteration: 69000 loss: 0.0031 lr: 1e-05
iteration: 70000 loss: 0.0031 lr: 1e-05
iteration: 71000 loss: 0.0030 lr: 1e-05
iteration: 72000 loss: 0.0030 lr: 1e-05
iteration: 73000 loss: 0.0031 lr: 1e-05
iteration: 74000 loss: 0.0030 lr: 1e-05
iteration: 75000 loss: 0.0030 lr: 1e-05
iteration: 76000 loss: 0.0030 lr: 1e-05
iteration: 77000 loss: 0.0029 lr: 1e-05
iteration: 78000 loss: 0.0029 lr: 1e-05
iteration: 79000 loss: 0.0029 lr: 1e-05
iteration: 80000 loss: 0.0030 lr: 1e-05
iteration: 81000 loss: 0.0031 lr: 1e-05
iteration: 82000 loss: 0.0030 lr: 1e-05
iteration: 83000 loss: 0.0029 lr: 1e-05
iteration: 84000 loss: 0.0030 lr: 1e-05
iteration: 85000 loss: 0.0029 lr: 1e-05
iteration: 86000 loss: 0.0029 lr: 1e-05
iteration: 87000 loss: 0.0029 lr: 1e-05
iteration: 88000 loss: 0.0029 lr: 1e-05
iteration: 89000 loss: 0.0029 lr: 1e-05
iteration: 90000 loss: 0.0029 lr: 1e-05
iteration: 91000 loss: 0.0029 lr: 1e-05
iteration: 92000 loss: 0.0029 lr: 1e-05
iteration: 93000 loss: 0.0029 lr: 1e-05
iteration: 94000 loss: 0.0029 lr: 1e-05
iteration: 95000 loss: 0.0028 lr: 1e-05
iteration: 96000 loss: 0.0028 lr: 1e-05
iteration: 97000 loss: 0.0029 lr: 1e-05
iteration: 98000 loss: 0.0028 lr: 1e-05
iteration: 99000 loss: 0.0028 lr: 1e-05
iteration: 100000 loss: 0.0029 lr: 1e-05
iteration: 101000 loss: 0.0029 lr: 1e-05
iteration: 102000 loss: 0.0028 lr: 1e-05
iteration: 103000 loss: 0.0027 lr: 1e-05
iteration: 104000 loss: 0.0028 lr: 1e-05
iteration: 105000 loss: 0.0028 lr: 1e-05
iteration: 106000 loss: 0.0028 lr: 1e-05
iteration: 107000 loss: 0.0027 lr: 1e-05
iteration: 108000 loss: 0.0027 lr: 1e-05
iteration: 109000 loss: 0.0027 lr: 1e-05
iteration: 110000 loss: 0.0027 lr: 1e-05
iteration: 111000 loss: 0.0027 lr: 1e-05
iteration: 112000 loss: 0.0027 lr: 1e-05
iteration: 113000 loss: 0.0028 lr: 1e-05
iteration: 114000 loss: 0.0027 lr: 1e-05
iteration: 115000 loss: 0.0027 lr: 1e-05
iteration: 116000 loss: 0.0027 lr: 1e-05
iteration: 117000 loss: 0.0027 lr: 1e-05
iteration: 118000 loss: 0.0027 lr: 1e-05
iteration: 119000 loss: 0.0027 lr: 1e-05
iteration: 120000 loss: 0.0026 lr: 1e-05
iteration: 121000 loss: 0.0027 lr: 1e-05
iteration: 122000 loss: 0.0027 lr: 1e-05
iteration: 123000 loss: 0.0026 lr: 1e-05
iteration: 124000 loss: 0.0027 lr: 1e-05
iteration: 125000 loss: 0.0026 lr: 1e-05
iteration: 126000 loss: 0.0026 lr: 1e-05
iteration: 127000 loss: 0.0026 lr: 1e-05
iteration: 128000 loss: 0.0026 lr: 1e-05
iteration: 129000 loss: 0.0027 lr: 1e-05
iteration: 130000 loss: 0.0027 lr: 1e-05
iteration: 131000 loss: 0.0026 lr: 1e-05
iteration: 132000 loss: 0.0027 lr: 1e-05
iteration: 133000 loss: 0.0027 lr: 1e-05
iteration: 134000 loss: 0.0026 lr: 1e-05
iteration: 135000 loss: 0.0026 lr: 1e-05
iteration: 136000 loss: 0.0026 lr: 1e-05
iteration: 137000 loss: 0.0026 lr: 1e-05
iteration: 138000 loss: 0.0026 lr: 1e-05
iteration: 139000 loss: 0.0026 lr: 1e-05
iteration: 140000 loss: 0.0026 lr: 1e-05
iteration: 141000 loss: 0.0026 lr: 1e-05
iteration: 142000 loss: 0.0025 lr: 1e-05
iteration: 143000 loss: 0.0026 lr: 1e-05
iteration: 144000 loss: 0.0026 lr: 1e-05
iteration: 145000 loss: 0.0025 lr: 1e-05
iteration: 146000 loss: 0.0025 lr: 1e-05
iteration: 147000 loss: 0.0025 lr: 1e-05
iteration: 148000 loss: 0.0025 lr: 1e-05
iteration: 149000 loss: 0.0025 lr: 1e-05
iteration: 150000 loss: 0.0025 lr: 1e-05
iteration: 151000 loss: 0.0025 lr: 1e-05
iteration: 152000 loss: 0.0025 lr: 1e-05
iteration: 153000 loss: 0.0025 lr: 1e-05
iteration: 154000 loss: 0.0025 lr: 1e-05
iteration: 155000 loss: 0.0025 lr: 1e-05
iteration: 156000 loss: 0.0025 lr: 1e-05
iteration: 157000 loss: 0.0024 lr: 1e-05
iteration: 158000 loss: 0.0025 lr: 1e-05
iteration: 159000 loss: 0.0025 lr: 1e-05
iteration: 160000 loss: 0.0025 lr: 1e-05
iteration: 161000 loss: 0.0025 lr: 1e-05
iteration: 162000 loss: 0.0025 lr: 1e-05
iteration: 163000 loss: 0.0025 lr: 1e-05
iteration: 164000 loss: 0.0025 lr: 1e-05
iteration: 165000 loss: 0.0025 lr: 1e-05
iteration: 166000 loss: 0.0025 lr: 1e-05
iteration: 167000 loss: 0.0024 lr: 1e-05
iteration: 168000 loss: 0.0025 lr: 1e-05
iteration: 169000 loss: 0.0025 lr: 1e-05
iteration: 170000 loss: 0.0025 lr: 1e-05
iteration: 171000 loss: 0.0025 lr: 1e-05
iteration: 172000 loss: 0.0024 lr: 1e-05
iteration: 173000 loss: 0.0024 lr: 1e-05
iteration: 174000 loss: 0.0024 lr: 1e-05
iteration: 175000 loss: 0.0024 lr: 1e-05
iteration: 176000 loss: 0.0024 lr: 1e-05
iteration: 177000 loss: 0.0024 lr: 1e-05
iteration: 178000 loss: 0.0024 lr: 1e-05
iteration: 179000 loss: 0.0025 lr: 1e-05
iteration: 180000 loss: 0.0024 lr: 1e-05
iteration: 181000 loss: 0.0025 lr: 1e-05
iteration: 182000 loss: 0.0024 lr: 1e-05
iteration: 183000 loss: 0.0024 lr: 1e-05
iteration: 184000 loss: 0.0024 lr: 1e-05
iteration: 185000 loss: 0.0024 lr: 1e-05
iteration: 186000 loss: 0.0024 lr: 1e-05
iteration: 187000 loss: 0.0024 lr: 1e-05
iteration: 188000 loss: 0.0024 lr: 1e-05
iteration: 189000 loss: 0.0024 lr: 1e-05
iteration: 190000 loss: 0.0024 lr: 1e-05
iteration: 191000 loss: 0.0024 lr: 1e-05
iteration: 192000 loss: 0.0024 lr: 1e-05
iteration: 193000 loss: 0.0024 lr: 1e-05
iteration: 194000 loss: 0.0023 lr: 1e-05
iteration: 195000 loss: 0.0024 lr: 1e-05
iteration: 196000 loss: 0.0023 lr: 1e-05
iteration: 197000 loss: 0.0024 lr: 1e-05
iteration: 198000 loss: 0.0024 lr: 1e-05
iteration: 199000 loss: 0.0024 lr: 1e-05
iteration: 200000 loss: 0.0024 lr: 1e-05
iteration: 201000 loss: 0.0023 lr: 1e-05
iteration: 202000 loss: 0.0024 lr: 1e-05
iteration: 203000 loss: 0.0023 lr: 1e-05
iteration: 204000 loss: 0.0023 lr: 1e-05
iteration: 205000 loss: 0.0023 lr: 1e-05
iteration: 206000 loss: 0.0023 lr: 1e-05
iteration: 207000 loss: 0.0023 lr: 1e-05
iteration: 208000 loss: 0.0023 lr: 1e-05
iteration: 209000 loss: 0.0023 lr: 1e-05
iteration: 210000 loss: 0.0023 lr: 1e-05
iteration: 211000 loss: 0.0023 lr: 1e-05
iteration: 212000 loss: 0.0023 lr: 1e-05
iteration: 213000 loss: 0.0023 lr: 1e-05
iteration: 214000 loss: 0.0023 lr: 1e-05
iteration: 215000 loss: 0.0023 lr: 1e-05
iteration: 216000 loss: 0.0024 lr: 1e-05
iteration: 217000 loss: 0.0023 lr: 1e-05
iteration: 218000 loss: 0.0023 lr: 1e-05
iteration: 219000 loss: 0.0023 lr: 1e-05
iteration: 220000 loss: 0.0023 lr: 1e-05
iteration: 221000 loss: 0.0023 lr: 1e-05
iteration: 222000 loss: 0.0024 lr: 1e-05
iteration: 223000 loss: 0.0024 lr: 1e-05
iteration: 224000 loss: 0.0023 lr: 1e-05
iteration: 225000 loss: 0.0023 lr: 1e-05
iteration: 226000 loss: 0.0022 lr: 1e-05
iteration: 227000 loss: 0.0023 lr: 1e-05
iteration: 228000 loss: 0.0023 lr: 1e-05
iteration: 229000 loss: 0.0022 lr: 1e-05
iteration: 230000 loss: 0.0023 lr: 1e-05
iteration: 231000 loss: 0.0023 lr: 1e-05
iteration: 232000 loss: 0.0024 lr: 1e-05
iteration: 233000 loss: 0.0023 lr: 1e-05
iteration: 234000 loss: 0.0022 lr: 1e-05
iteration: 235000 loss: 0.0023 lr: 1e-05
iteration: 236000 loss: 0.0023 lr: 1e-05
iteration: 237000 loss: 0.0023 lr: 1e-05
iteration: 238000 loss: 0.0022 lr: 1e-05
iteration: 239000 loss: 0.0023 lr: 1e-05
iteration: 240000 loss: 0.0022 lr: 1e-05
iteration: 241000 loss: 0.0022 lr: 1e-05
iteration: 242000 loss: 0.0022 lr: 1e-05
iteration: 243000 loss: 0.0023 lr: 1e-05
iteration: 244000 loss: 0.0023 lr: 1e-05
iteration: 245000 loss: 0.0022 lr: 1e-05
iteration: 246000 loss: 0.0023 lr: 1e-05
iteration: 247000 loss: 0.0022 lr: 1e-05
iteration: 248000 loss: 0.0023 lr: 1e-05
iteration: 249000 loss: 0.0022 lr: 1e-05
iteration: 250000 loss: 0.0022 lr: 1e-05
iteration: 251000 loss: 0.0022 lr: 1e-05
iteration: 252000 loss: 0.0022 lr: 1e-05
iteration: 253000 loss: 0.0023 lr: 1e-05
iteration: 254000 loss: 0.0022 lr: 1e-05
iteration: 255000 loss: 0.0022 lr: 1e-05
iteration: 256000 loss: 0.0022 lr: 1e-05
iteration: 257000 loss: 0.0022 lr: 1e-05
iteration: 258000 loss: 0.0022 lr: 1e-05
iteration: 259000 loss: 0.0022 lr: 1e-05
iteration: 260000 loss: 0.0022 lr: 1e-05
iteration: 261000 loss: 0.0022 lr: 1e-05
iteration: 262000 loss: 0.0022 lr: 1e-05
iteration: 263000 loss: 0.0021 lr: 1e-05
iteration: 264000 loss: 0.0022 lr: 1e-05
iteration: 265000 loss: 0.0022 lr: 1e-05
iteration: 266000 loss: 0.0022 lr: 1e-05
iteration: 267000 loss: 0.0022 lr: 1e-05
iteration: 268000 loss: 0.0022 lr: 1e-05
iteration: 269000 loss: 0.0022 lr: 1e-05
iteration: 270000 loss: 0.0022 lr: 1e-05
iteration: 271000 loss: 0.0022 lr: 1e-05
iteration: 272000 loss: 0.0021 lr: 1e-05
iteration: 273000 loss: 0.0022 lr: 1e-05
iteration: 274000 loss: 0.0022 lr: 1e-05
iteration: 275000 loss: 0.0022 lr: 1e-05
iteration: 276000 loss: 0.0022 lr: 1e-05
iteration: 277000 loss: 0.0022 lr: 1e-05
iteration: 278000 loss: 0.0022 lr: 1e-05
iteration: 279000 loss: 0.0021 lr: 1e-05
iteration: 280000 loss: 0.0022 lr: 1e-05
iteration: 281000 loss: 0.0022 lr: 1e-05
iteration: 282000 loss: 0.0022 lr: 1e-05
iteration: 283000 loss: 0.0022 lr: 1e-05
iteration: 284000 loss: 0.0022 lr: 1e-05
iteration: 285000 loss: 0.0022 lr: 1e-05
iteration: 286000 loss: 0.0022 lr: 1e-05
iteration: 287000 loss: 0.0021 lr: 1e-05
iteration: 288000 loss: 0.0022 lr: 1e-05
iteration: 289000 loss: 0.0021 lr: 1e-05
iteration: 290000 loss: 0.0022 lr: 1e-05
iteration: 291000 loss: 0.0021 lr: 1e-05
iteration: 292000 loss: 0.0021 lr: 1e-05
iteration: 293000 loss: 0.0021 lr: 1e-05
iteration: 294000 loss: 0.0022 lr: 1e-05
iteration: 295000 loss: 0.0021 lr: 1e-05
iteration: 296000 loss: 0.0021 lr: 1e-05
iteration: 297000 loss: 0.0022 lr: 1e-05
iteration: 298000 loss: 0.0021 lr: 1e-05
iteration: 299000 loss: 0.0021 lr: 1e-05
iteration: 300000 loss: 0.0022 lr: 1e-05
iteration: 301000 loss: 0.0022 lr: 1e-05
iteration: 302000 loss: 0.0021 lr: 1e-05
iteration: 303000 loss: 0.0021 lr: 1e-05
iteration: 304000 loss: 0.0021 lr: 1e-05
iteration: 305000 loss: 0.0022 lr: 1e-05
iteration: 306000 loss: 0.0021 lr: 1e-05
iteration: 307000 loss: 0.0021 lr: 1e-05
iteration: 308000 loss: 0.0021 lr: 1e-05
iteration: 309000 loss: 0.0022 lr: 1e-05
iteration: 310000 loss: 0.0021 lr: 1e-05
iteration: 311000 loss: 0.0021 lr: 1e-05
iteration: 312000 loss: 0.0021 lr: 1e-05
iteration: 313000 loss: 0.0022 lr: 1e-05
iteration: 314000 loss: 0.0021 lr: 1e-05
iteration: 315000 loss: 0.0021 lr: 1e-05
iteration: 316000 loss: 0.0022 lr: 1e-05
iteration: 317000 loss: 0.0021 lr: 1e-05
iteration: 318000 loss: 0.0022 lr: 1e-05
iteration: 319000 loss: 0.0021 lr: 1e-05
iteration: 320000 loss: 0.0021 lr: 1e-05
iteration: 321000 loss: 0.0021 lr: 1e-05
iteration: 322000 loss: 0.0021 lr: 1e-05
iteration: 323000 loss: 0.0021 lr: 1e-05
iteration: 324000 loss: 0.0021 lr: 1e-05
iteration: 325000 loss: 0.0021 lr: 1e-05
iteration: 326000 loss: 0.0021 lr: 1e-05
iteration: 327000 loss: 0.0021 lr: 1e-05
iteration: 328000 loss: 0.0021 lr: 1e-05
iteration: 329000 loss: 0.0021 lr: 1e-05
iteration: 330000 loss: 0.0021 lr: 1e-05
iteration: 331000 loss: 0.0021 lr: 1e-05
iteration: 332000 loss: 0.0021 lr: 1e-05
iteration: 333000 loss: 0.0021 lr: 1e-05
iteration: 334000 loss: 0.0021 lr: 1e-05
iteration: 335000 loss: 0.0021 lr: 1e-05
iteration: 336000 loss: 0.0021 lr: 1e-05
iteration: 337000 loss: 0.0022 lr: 1e-05
iteration: 338000 loss: 0.0021 lr: 1e-05
iteration: 339000 loss: 0.0021 lr: 1e-05
iteration: 340000 loss: 0.0021 lr: 1e-05
iteration: 341000 loss: 0.0021 lr: 1e-05
iteration: 342000 loss: 0.0021 lr: 1e-05
iteration: 343000 loss: 0.0021 lr: 1e-05
iteration: 344000 loss: 0.0021 lr: 1e-05
iteration: 345000 loss: 0.0020 lr: 1e-05
iteration: 346000 loss: 0.0020 lr: 1e-05
iteration: 347000 loss: 0.0021 lr: 1e-05
iteration: 348000 loss: 0.0021 lr: 1e-05
iteration: 349000 loss: 0.0021 lr: 1e-05
iteration: 350000 loss: 0.0021 lr: 1e-05
iteration: 351000 loss: 0.0020 lr: 1e-05
iteration: 352000 loss: 0.0020 lr: 1e-05
iteration: 353000 loss: 0.0021 lr: 1e-05
iteration: 354000 loss: 0.0020 lr: 1e-05
iteration: 355000 loss: 0.0021 lr: 1e-05
iteration: 356000 loss: 0.0021 lr: 1e-05
iteration: 357000 loss: 0.0021 lr: 1e-05
iteration: 358000 loss: 0.0021 lr: 1e-05
iteration: 359000 loss: 0.0020 lr: 1e-05
iteration: 360000 loss: 0.0021 lr: 1e-05
iteration: 361000 loss: 0.0021 lr: 1e-05
iteration: 362000 loss: 0.0020 lr: 1e-05
iteration: 363000 loss: 0.0021 lr: 1e-05
iteration: 364000 loss: 0.0022 lr: 1e-05
iteration: 365000 loss: 0.0021 lr: 1e-05
iteration: 366000 loss: 0.0021 lr: 1e-05
iteration: 367000 loss: 0.0020 lr: 1e-05
iteration: 368000 loss: 0.0021 lr: 1e-05
iteration: 369000 loss: 0.0020 lr: 1e-05
iteration: 370000 loss: 0.0021 lr: 1e-05
iteration: 371000 loss: 0.0020 lr: 1e-05
iteration: 372000 loss: 0.0020 lr: 1e-05
iteration: 373000 loss: 0.0021 lr: 1e-05
iteration: 374000 loss: 0.0020 lr: 1e-05
iteration: 375000 loss: 0.0020 lr: 1e-05
iteration: 376000 loss: 0.0020 lr: 1e-05
iteration: 377000 loss: 0.0021 lr: 1e-05
iteration: 378000 loss: 0.0020 lr: 1e-05
iteration: 379000 loss: 0.0020 lr: 1e-05
iteration: 380000 loss: 0.0020 lr: 1e-05
iteration: 381000 loss: 0.0020 lr: 1e-05
iteration: 382000 loss: 0.0020 lr: 1e-05
iteration: 383000 loss: 0.0020 lr: 1e-05
iteration: 384000 loss: 0.0021 lr: 1e-05
iteration: 385000 loss: 0.0020 lr: 1e-05
iteration: 386000 loss: 0.0020 lr: 1e-05
iteration: 387000 loss: 0.0020 lr: 1e-05
iteration: 388000 loss: 0.0020 lr: 1e-05
iteration: 389000 loss: 0.0020 lr: 1e-05
iteration: 390000 loss: 0.0020 lr: 1e-05
iteration: 391000 loss: 0.0020 lr: 1e-05
iteration: 392000 loss: 0.0020 lr: 1e-05
iteration: 393000 loss: 0.0020 lr: 1e-05
iteration: 394000 loss: 0.0020 lr: 1e-05
iteration: 395000 loss: 0.0020 lr: 1e-05
iteration: 396000 loss: 0.0020 lr: 1e-05
iteration: 397000 loss: 0.0020 lr: 1e-05
iteration: 398000 loss: 0.0020 lr: 1e-05
iteration: 399000 loss: 0.0020 lr: 1e-05
iteration: 400000 loss: 0.0020 lr: 1e-05
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/core/train.py", line 83, in load_and_enqueue
    sess.run(enqueue_op, feed_dict=food)
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1115, in _run
    raise RuntimeError('Attempted to use a closed Session.')
RuntimeError: Attempted to use a closed Session.
Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15]],
 'all_joints_names': ['t3L',
                      'wstL',
                      't5L',
                      'elbL',
                      'shdL',
                      'ankL',
                      'nl',
                      'str',
                      'lmb',
                      'shdR',
                      'ankR',
                      'elbR',
                      'wstR',
                      't5R',
                      't3R',
                      'tail'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 8,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC84shuffle2.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'fliplr': False,
 'gaussian_blur': False,
 'gaussian_blur_params': {'sigma': [0.5, 4.0]},
 'global_scale': 0.8,
 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_84shuffle2.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]],
 'net_type': 'resnet_101',
 'num_joints': 16,
 'optimizer': 'adam',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset84shuffle2/train/snapshot',
 'stride': 8.0,
 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]],
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2022-07-30 15:02:37.295035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22344 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:67:00.0, compute capability: 8.6
iteration: 1000 loss: 0.0288 lr: 0.0001
iteration: 2000 loss: 0.0146 lr: 0.0001
iteration: 3000 loss: 0.0119 lr: 0.0001
iteration: 4000 loss: 0.0109 lr: 0.0001
iteration: 5000 loss: 0.0097 lr: 0.0001
iteration: 6000 loss: 0.0091 lr: 0.0001
iteration: 7000 loss: 0.0087 lr: 0.0001
iteration: 8000 loss: 0.0075 lr: 5e-05
iteration: 9000 loss: 0.0067 lr: 5e-05
iteration: 10000 loss: 0.0063 lr: 5e-05
iteration: 11000 loss: 0.0062 lr: 5e-05
iteration: 12000 loss: 0.0060 lr: 5e-05
iteration: 13000 loss: 0.0052 lr: 1e-05
iteration: 14000 loss: 0.0050 lr: 1e-05
iteration: 15000 loss: 0.0049 lr: 1e-05
iteration: 16000 loss: 0.0049 lr: 1e-05
iteration: 17000 loss: 0.0047 lr: 1e-05
iteration: 18000 loss: 0.0047 lr: 1e-05
iteration: 19000 loss: 0.0047 lr: 1e-05
iteration: 20000 loss: 0.0046 lr: 1e-05
iteration: 21000 loss: 0.0045 lr: 1e-05
iteration: 22000 loss: 0.0044 lr: 1e-05
iteration: 23000 loss: 0.0044 lr: 1e-05
iteration: 24000 loss: 0.0044 lr: 1e-05
iteration: 25000 loss: 0.0043 lr: 1e-05
iteration: 26000 loss: 0.0042 lr: 1e-05
iteration: 27000 loss: 0.0041 lr: 1e-05
iteration: 28000 loss: 0.0041 lr: 1e-05
iteration: 29000 loss: 0.0041 lr: 1e-05
iteration: 30000 loss: 0.0041 lr: 1e-05
iteration: 31000 loss: 0.0040 lr: 1e-05
iteration: 32000 loss: 0.0039 lr: 1e-05
iteration: 33000 loss: 0.0040 lr: 1e-05
iteration: 34000 loss: 0.0039 lr: 1e-05
iteration: 35000 loss: 0.0039 lr: 1e-05
iteration: 36000 loss: 0.0039 lr: 1e-05
iteration: 37000 loss: 0.0038 lr: 1e-05
iteration: 38000 loss: 0.0038 lr: 1e-05
iteration: 39000 loss: 0.0038 lr: 1e-05
iteration: 40000 loss: 0.0038 lr: 1e-05
iteration: 41000 loss: 0.0038 lr: 1e-05
iteration: 42000 loss: 0.0037 lr: 1e-05
iteration: 43000 loss: 0.0037 lr: 1e-05
iteration: 44000 loss: 0.0037 lr: 1e-05
iteration: 45000 loss: 0.0037 lr: 1e-05
iteration: 46000 loss: 0.0036 lr: 1e-05
iteration: 47000 loss: 0.0036 lr: 1e-05
iteration: 48000 loss: 0.0036 lr: 1e-05
iteration: 49000 loss: 0.0035 lr: 1e-05
iteration: 50000 loss: 0.0035 lr: 1e-05
iteration: 51000 loss: 0.0035 lr: 1e-05
iteration: 52000 loss: 0.0035 lr: 1e-05
iteration: 53000 loss: 0.0035 lr: 1e-05
iteration: 54000 loss: 0.0034 lr: 1e-05
iteration: 55000 loss: 0.0035 lr: 1e-05
iteration: 56000 loss: 0.0034 lr: 1e-05
iteration: 57000 loss: 0.0034 lr: 1e-05
iteration: 58000 loss: 0.0033 lr: 1e-05
iteration: 59000 loss: 0.0034 lr: 1e-05
iteration: 60000 loss: 0.0033 lr: 1e-05
iteration: 61000 loss: 0.0033 lr: 1e-05
iteration: 62000 loss: 0.0034 lr: 1e-05
iteration: 63000 loss: 0.0033 lr: 1e-05
iteration: 64000 loss: 0.0033 lr: 1e-05
iteration: 65000 loss: 0.0033 lr: 1e-05
iteration: 66000 loss: 0.0033 lr: 1e-05
iteration: 67000 loss: 0.0033 lr: 1e-05
iteration: 68000 loss: 0.0032 lr: 1e-05
iteration: 69000 loss: 0.0032 lr: 1e-05
iteration: 70000 loss: 0.0033 lr: 1e-05
iteration: 71000 loss: 0.0032 lr: 1e-05
iteration: 72000 loss: 0.0032 lr: 1e-05
iteration: 73000 loss: 0.0032 lr: 1e-05
iteration: 74000 loss: 0.0032 lr: 1e-05
iteration: 75000 loss: 0.0031 lr: 1e-05
iteration: 76000 loss: 0.0031 lr: 1e-05
iteration: 77000 loss: 0.0031 lr: 1e-05
iteration: 78000 loss: 0.0032 lr: 1e-05
iteration: 79000 loss: 0.0031 lr: 1e-05
iteration: 80000 loss: 0.0031 lr: 1e-05
iteration: 81000 loss: 0.0031 lr: 1e-05
iteration: 82000 loss: 0.0030 lr: 1e-05
iteration: 83000 loss: 0.0031 lr: 1e-05
iteration: 84000 loss: 0.0030 lr: 1e-05
iteration: 85000 loss: 0.0030 lr: 1e-05
iteration: 86000 loss: 0.0030 lr: 1e-05
iteration: 87000 loss: 0.0030 lr: 1e-05
iteration: 88000 loss: 0.0030 lr: 1e-05
iteration: 89000 loss: 0.0030 lr: 1e-05
iteration: 90000 loss: 0.0030 lr: 1e-05
iteration: 91000 loss: 0.0030 lr: 1e-05
iteration: 92000 loss: 0.0029 lr: 1e-05
iteration: 93000 loss: 0.0030 lr: 1e-05
iteration: 94000 loss: 0.0030 lr: 1e-05
iteration: 95000 loss: 0.0029 lr: 1e-05
iteration: 96000 loss: 0.0030 lr: 1e-05
iteration: 97000 loss: 0.0029 lr: 1e-05
iteration: 98000 loss: 0.0029 lr: 1e-05
iteration: 99000 loss: 0.0030 lr: 1e-05
iteration: 100000 loss: 0.0030 lr: 1e-05
iteration: 101000 loss: 0.0029 lr: 1e-05
iteration: 102000 loss: 0.0029 lr: 1e-05
iteration: 103000 loss: 0.0028 lr: 1e-05
iteration: 104000 loss: 0.0028 lr: 1e-05
iteration: 105000 loss: 0.0029 lr: 1e-05
iteration: 106000 loss: 0.0029 lr: 1e-05
iteration: 107000 loss: 0.0028 lr: 1e-05
iteration: 108000 loss: 0.0028 lr: 1e-05
iteration: 109000 loss: 0.0028 lr: 1e-05
iteration: 110000 loss: 0.0029 lr: 1e-05
iteration: 111000 loss: 0.0028 lr: 1e-05
iteration: 112000 loss: 0.0028 lr: 1e-05
iteration: 113000 loss: 0.0028 lr: 1e-05
iteration: 114000 loss: 0.0028 lr: 1e-05
iteration: 115000 loss: 0.0028 lr: 1e-05
iteration: 116000 loss: 0.0028 lr: 1e-05
iteration: 117000 loss: 0.0028 lr: 1e-05
iteration: 118000 loss: 0.0028 lr: 1e-05
iteration: 119000 loss: 0.0028 lr: 1e-05
iteration: 120000 loss: 0.0028 lr: 1e-05
iteration: 121000 loss: 0.0028 lr: 1e-05
iteration: 122000 loss: 0.0028 lr: 1e-05
iteration: 123000 loss: 0.0027 lr: 1e-05
iteration: 124000 loss: 0.0027 lr: 1e-05
iteration: 125000 loss: 0.0027 lr: 1e-05
iteration: 126000 loss: 0.0027 lr: 1e-05
iteration: 127000 loss: 0.0027 lr: 1e-05
iteration: 128000 loss: 0.0027 lr: 1e-05
iteration: 129000 loss: 0.0027 lr: 1e-05
iteration: 130000 loss: 0.0028 lr: 1e-05
iteration: 131000 loss: 0.0026 lr: 1e-05
iteration: 132000 loss: 0.0027 lr: 1e-05
iteration: 133000 loss: 0.0027 lr: 1e-05
iteration: 134000 loss: 0.0027 lr: 1e-05
iteration: 135000 loss: 0.0026 lr: 1e-05
iteration: 136000 loss: 0.0026 lr: 1e-05
iteration: 137000 loss: 0.0026 lr: 1e-05
iteration: 138000 loss: 0.0026 lr: 1e-05
iteration: 139000 loss: 0.0027 lr: 1e-05
iteration: 140000 loss: 0.0026 lr: 1e-05
iteration: 141000 loss: 0.0026 lr: 1e-05
iteration: 142000 loss: 0.0026 lr: 1e-05
iteration: 143000 loss: 0.0026 lr: 1e-05
iteration: 144000 loss: 0.0026 lr: 1e-05
iteration: 145000 loss: 0.0026 lr: 1e-05
iteration: 146000 loss: 0.0026 lr: 1e-05
iteration: 147000 loss: 0.0026 lr: 1e-05
iteration: 148000 loss: 0.0026 lr: 1e-05
iteration: 149000 loss: 0.0026 lr: 1e-05
iteration: 150000 loss: 0.0026 lr: 1e-05
iteration: 151000 loss: 0.0026 lr: 1e-05
iteration: 152000 loss: 0.0026 lr: 1e-05
iteration: 153000 loss: 0.0027 lr: 1e-05
iteration: 154000 loss: 0.0026 lr: 1e-05
iteration: 155000 loss: 0.0027 lr: 1e-05
iteration: 156000 loss: 0.0025 lr: 1e-05
iteration: 157000 loss: 0.0026 lr: 1e-05
iteration: 158000 loss: 0.0026 lr: 1e-05
iteration: 159000 loss: 0.0025 lr: 1e-05
iteration: 160000 loss: 0.0025 lr: 1e-05
iteration: 161000 loss: 0.0026 lr: 1e-05
iteration: 162000 loss: 0.0025 lr: 1e-05
iteration: 163000 loss: 0.0026 lr: 1e-05
iteration: 164000 loss: 0.0025 lr: 1e-05
iteration: 165000 loss: 0.0026 lr: 1e-05
iteration: 166000 loss: 0.0025 lr: 1e-05
iteration: 167000 loss: 0.0025 lr: 1e-05
iteration: 168000 loss: 0.0025 lr: 1e-05
iteration: 169000 loss: 0.0025 lr: 1e-05
iteration: 170000 loss: 0.0026 lr: 1e-05
iteration: 171000 loss: 0.0024 lr: 1e-05
iteration: 172000 loss: 0.0024 lr: 1e-05
iteration: 173000 loss: 0.0025 lr: 1e-05
iteration: 174000 loss: 0.0025 lr: 1e-05
iteration: 175000 loss: 0.0025 lr: 1e-05
iteration: 176000 loss: 0.0025 lr: 1e-05
iteration: 177000 loss: 0.0025 lr: 1e-05
iteration: 178000 loss: 0.0024 lr: 1e-05
iteration: 179000 loss: 0.0024 lr: 1e-05
iteration: 180000 loss: 0.0025 lr: 1e-05
iteration: 181000 loss: 0.0025 lr: 1e-05
iteration: 182000 loss: 0.0024 lr: 1e-05
iteration: 183000 loss: 0.0024 lr: 1e-05
iteration: 184000 loss: 0.0025 lr: 1e-05
iteration: 185000 loss: 0.0025 lr: 1e-05
iteration: 186000 loss: 0.0024 lr: 1e-05
iteration: 187000 loss: 0.0024 lr: 1e-05
iteration: 188000 loss: 0.0024 lr: 1e-05
iteration: 189000 loss: 0.0024 lr: 1e-05
iteration: 190000 loss: 0.0024 lr: 1e-05
iteration: 191000 loss: 0.0024 lr: 1e-05
iteration: 192000 loss: 0.0024 lr: 1e-05
iteration: 193000 loss: 0.0024 lr: 1e-05
iteration: 194000 loss: 0.0024 lr: 1e-05
iteration: 195000 loss: 0.0024 lr: 1e-05
iteration: 196000 loss: 0.0023 lr: 1e-05
iteration: 197000 loss: 0.0025 lr: 1e-05
iteration: 198000 loss: 0.0024 lr: 1e-05
iteration: 199000 loss: 0.0024 lr: 1e-05
iteration: 200000 loss: 0.0024 lr: 1e-05
iteration: 201000 loss: 0.0024 lr: 1e-05
iteration: 202000 loss: 0.0024 lr: 1e-05
iteration: 203000 loss: 0.0024 lr: 1e-05
iteration: 204000 loss: 0.0023 lr: 1e-05
iteration: 205000 loss: 0.0023 lr: 1e-05
iteration: 206000 loss: 0.0024 lr: 1e-05
iteration: 207000 loss: 0.0023 lr: 1e-05
iteration: 208000 loss: 0.0024 lr: 1e-05
iteration: 209000 loss: 0.0024 lr: 1e-05
iteration: 210000 loss: 0.0023 lr: 1e-05
iteration: 211000 loss: 0.0024 lr: 1e-05
iteration: 212000 loss: 0.0023 lr: 1e-05
iteration: 213000 loss: 0.0024 lr: 1e-05
iteration: 214000 loss: 0.0024 lr: 1e-05
iteration: 215000 loss: 0.0024 lr: 1e-05
iteration: 216000 loss: 0.0024 lr: 1e-05
iteration: 217000 loss: 0.0024 lr: 1e-05
iteration: 218000 loss: 0.0023 lr: 1e-05
iteration: 219000 loss: 0.0024 lr: 1e-05
iteration: 220000 loss: 0.0023 lr: 1e-05
iteration: 221000 loss: 0.0023 lr: 1e-05
iteration: 222000 loss: 0.0023 lr: 1e-05
iteration: 223000 loss: 0.0023 lr: 1e-05
iteration: 224000 loss: 0.0023 lr: 1e-05
iteration: 225000 loss: 0.0024 lr: 1e-05
iteration: 226000 loss: 0.0023 lr: 1e-05
iteration: 227000 loss: 0.0023 lr: 1e-05
iteration: 228000 loss: 0.0024 lr: 1e-05
iteration: 229000 loss: 0.0023 lr: 1e-05
iteration: 230000 loss: 0.0023 lr: 1e-05
iteration: 231000 loss: 0.0023 lr: 1e-05
iteration: 232000 loss: 0.0023 lr: 1e-05
iteration: 233000 loss: 0.0023 lr: 1e-05
iteration: 234000 loss: 0.0023 lr: 1e-05
iteration: 235000 loss: 0.0023 lr: 1e-05
iteration: 236000 loss: 0.0023 lr: 1e-05
iteration: 237000 loss: 0.0023 lr: 1e-05
iteration: 238000 loss: 0.0023 lr: 1e-05
iteration: 239000 loss: 0.0023 lr: 1e-05
iteration: 240000 loss: 0.0022 lr: 1e-05
iteration: 241000 loss: 0.0023 lr: 1e-05
iteration: 242000 loss: 0.0023 lr: 1e-05
iteration: 243000 loss: 0.0023 lr: 1e-05
iteration: 244000 loss: 0.0023 lr: 1e-05
iteration: 245000 loss: 0.0022 lr: 1e-05
iteration: 246000 loss: 0.0022 lr: 1e-05
iteration: 247000 loss: 0.0023 lr: 1e-05
iteration: 248000 loss: 0.0023 lr: 1e-05
iteration: 249000 loss: 0.0022 lr: 1e-05
iteration: 250000 loss: 0.0022 lr: 1e-05
iteration: 251000 loss: 0.0022 lr: 1e-05
iteration: 252000 loss: 0.0023 lr: 1e-05
iteration: 253000 loss: 0.0022 lr: 1e-05
iteration: 254000 loss: 0.0023 lr: 1e-05
iteration: 255000 loss: 0.0022 lr: 1e-05
iteration: 256000 loss: 0.0022 lr: 1e-05
iteration: 257000 loss: 0.0022 lr: 1e-05
iteration: 258000 loss: 0.0022 lr: 1e-05
iteration: 259000 loss: 0.0022 lr: 1e-05
iteration: 260000 loss: 0.0022 lr: 1e-05
iteration: 261000 loss: 0.0022 lr: 1e-05
iteration: 262000 loss: 0.0023 lr: 1e-05
iteration: 263000 loss: 0.0022 lr: 1e-05
iteration: 264000 loss: 0.0022 lr: 1e-05
iteration: 265000 loss: 0.0022 lr: 1e-05
iteration: 266000 loss: 0.0022 lr: 1e-05
iteration: 267000 loss: 0.0022 lr: 1e-05
iteration: 268000 loss: 0.0022 lr: 1e-05
iteration: 269000 loss: 0.0022 lr: 1e-05
iteration: 270000 loss: 0.0022 lr: 1e-05
iteration: 271000 loss: 0.0022 lr: 1e-05
iteration: 272000 loss: 0.0022 lr: 1e-05
iteration: 273000 loss: 0.0022 lr: 1e-05
iteration: 274000 loss: 0.0022 lr: 1e-05
iteration: 275000 loss: 0.0022 lr: 1e-05
iteration: 276000 loss: 0.0022 lr: 1e-05
iteration: 277000 loss: 0.0022 lr: 1e-05
iteration: 278000 loss: 0.0022 lr: 1e-05
iteration: 279000 loss: 0.0022 lr: 1e-05
iteration: 280000 loss: 0.0022 lr: 1e-05
iteration: 281000 loss: 0.0022 lr: 1e-05
iteration: 282000 loss: 0.0022 lr: 1e-05
iteration: 283000 loss: 0.0022 lr: 1e-05
iteration: 284000 loss: 0.0022 lr: 1e-05
iteration: 285000 loss: 0.0022 lr: 1e-05
iteration: 286000 loss: 0.0022 lr: 1e-05
iteration: 287000 loss: 0.0022 lr: 1e-05
iteration: 288000 loss: 0.0022 lr: 1e-05
iteration: 289000 loss: 0.0021 lr: 1e-05
iteration: 290000 loss: 0.0022 lr: 1e-05
iteration: 291000 loss: 0.0022 lr: 1e-05
iteration: 292000 loss: 0.0023 lr: 1e-05
iteration: 293000 loss: 0.0022 lr: 1e-05
iteration: 294000 loss: 0.0021 lr: 1e-05
iteration: 295000 loss: 0.0022 lr: 1e-05
iteration: 296000 loss: 0.0022 lr: 1e-05
iteration: 297000 loss: 0.0021 lr: 1e-05
iteration: 298000 loss: 0.0022 lr: 1e-05
iteration: 299000 loss: 0.0022 lr: 1e-05
iteration: 300000 loss: 0.0021 lr: 1e-05
iteration: 301000 loss: 0.0021 lr: 1e-05
iteration: 302000 loss: 0.0021 lr: 1e-05
iteration: 303000 loss: 0.0021 lr: 1e-05
iteration: 304000 loss: 0.0021 lr: 1e-05
iteration: 305000 loss: 0.0022 lr: 1e-05
iteration: 306000 loss: 0.0021 lr: 1e-05
iteration: 307000 loss: 0.0022 lr: 1e-05
iteration: 308000 loss: 0.0021 lr: 1e-05
iteration: 309000 loss: 0.0021 lr: 1e-05
iteration: 310000 loss: 0.0021 lr: 1e-05
iteration: 311000 loss: 0.0022 lr: 1e-05
iteration: 312000 loss: 0.0021 lr: 1e-05
iteration: 313000 loss: 0.0021 lr: 1e-05
iteration: 314000 loss: 0.0022 lr: 1e-05
iteration: 315000 loss: 0.0021 lr: 1e-05
iteration: 316000 loss: 0.0021 lr: 1e-05
iteration: 317000 loss: 0.0021 lr: 1e-05
iteration: 318000 loss: 0.0021 lr: 1e-05
iteration: 319000 loss: 0.0022 lr: 1e-05
iteration: 320000 loss: 0.0021 lr: 1e-05
iteration: 321000 loss: 0.0021 lr: 1e-05
iteration: 322000 loss: 0.0021 lr: 1e-05
iteration: 323000 loss: 0.0022 lr: 1e-05
iteration: 324000 loss: 0.0021 lr: 1e-05
iteration: 325000 loss: 0.0021 lr: 1e-05
iteration: 326000 loss: 0.0021 lr: 1e-05
iteration: 327000 loss: 0.0021 lr: 1e-05
iteration: 328000 loss: 0.0021 lr: 1e-05
iteration: 329000 loss: 0.0021 lr: 1e-05
iteration: 330000 loss: 0.0021 lr: 1e-05
iteration: 331000 loss: 0.0021 lr: 1e-05
iteration: 332000 loss: 0.0021 lr: 1e-05
iteration: 333000 loss: 0.0021 lr: 1e-05
iteration: 334000 loss: 0.0021 lr: 1e-05
iteration: 335000 loss: 0.0021 lr: 1e-05
iteration: 336000 loss: 0.0021 lr: 1e-05
iteration: 337000 loss: 0.0021 lr: 1e-05
iteration: 338000 loss: 0.0021 lr: 1e-05
iteration: 339000 loss: 0.0021 lr: 1e-05
iteration: 340000 loss: 0.0021 lr: 1e-05
iteration: 341000 loss: 0.0021 lr: 1e-05
iteration: 342000 loss: 0.0022 lr: 1e-05
iteration: 343000 loss: 0.0021 lr: 1e-05
iteration: 344000 loss: 0.0021 lr: 1e-05
iteration: 345000 loss: 0.0020 lr: 1e-05
iteration: 346000 loss: 0.0021 lr: 1e-05
iteration: 347000 loss: 0.0021 lr: 1e-05
iteration: 348000 loss: 0.0022 lr: 1e-05
iteration: 349000 loss: 0.0020 lr: 1e-05
iteration: 350000 loss: 0.0021 lr: 1e-05
iteration: 351000 loss: 0.0020 lr: 1e-05
iteration: 352000 loss: 0.0022 lr: 1e-05
iteration: 353000 loss: 0.0020 lr: 1e-05
iteration: 354000 loss: 0.0021 lr: 1e-05
iteration: 355000 loss: 0.0021 lr: 1e-05
iteration: 356000 loss: 0.0020 lr: 1e-05
iteration: 357000 loss: 0.0020 lr: 1e-05
iteration: 358000 loss: 0.0021 lr: 1e-05
iteration: 359000 loss: 0.0021 lr: 1e-05
iteration: 360000 loss: 0.0021 lr: 1e-05
iteration: 361000 loss: 0.0021 lr: 1e-05
iteration: 362000 loss: 0.0020 lr: 1e-05
iteration: 363000 loss: 0.0020 lr: 1e-05
iteration: 364000 loss: 0.0021 lr: 1e-05
iteration: 365000 loss: 0.0021 lr: 1e-05
iteration: 366000 loss: 0.0020 lr: 1e-05
iteration: 367000 loss: 0.0020 lr: 1e-05
iteration: 368000 loss: 0.0020 lr: 1e-05
iteration: 369000 loss: 0.0021 lr: 1e-05
iteration: 370000 loss: 0.0021 lr: 1e-05
iteration: 371000 loss: 0.0021 lr: 1e-05
iteration: 372000 loss: 0.0021 lr: 1e-05
iteration: 373000 loss: 0.0021 lr: 1e-05
iteration: 374000 loss: 0.0020 lr: 1e-05
iteration: 375000 loss: 0.0020 lr: 1e-05
iteration: 376000 loss: 0.0020 lr: 1e-05
iteration: 377000 loss: 0.0020 lr: 1e-05
iteration: 378000 loss: 0.0021 lr: 1e-05
iteration: 379000 loss: 0.0021 lr: 1e-05
iteration: 380000 loss: 0.0021 lr: 1e-05
iteration: 381000 loss: 0.0021 lr: 1e-05
iteration: 382000 loss: 0.0020 lr: 1e-05
iteration: 383000 loss: 0.0020 lr: 1e-05
iteration: 384000 loss: 0.0020 lr: 1e-05
iteration: 385000 loss: 0.0020 lr: 1e-05
iteration: 386000 loss: 0.0020 lr: 1e-05
iteration: 387000 loss: 0.0020 lr: 1e-05
iteration: 388000 loss: 0.0020 lr: 1e-05
iteration: 389000 loss: 0.0020 lr: 1e-05
iteration: 390000 loss: 0.0020 lr: 1e-05
iteration: 391000 loss: 0.0020 lr: 1e-05
iteration: 392000 loss: 0.0020 lr: 1e-05
iteration: 393000 loss: 0.0020 lr: 1e-05
iteration: 394000 loss: 0.0020 lr: 1e-05
iteration: 395000 loss: 0.0020 lr: 1e-05
iteration: 396000 loss: 0.0020 lr: 1e-05
iteration: 397000 loss: 0.0020 lr: 1e-05
iteration: 398000 loss: 0.0020 lr: 1e-05
iteration: 399000 loss: 0.0021 lr: 1e-05
iteration: 400000 loss: 0.0021 lr: 1e-05
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/core/train.py", line 83, in load_and_enqueue
    sess.run(enqueue_op, feed_dict=food)
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1115, in _run
    raise RuntimeError('Attempted to use a closed Session.')
RuntimeError: Attempted to use a closed Session.
Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15]],
 'all_joints_names': ['t3L',
                      'wstL',
                      't5L',
                      'elbL',
                      'shdL',
                      'ankL',
                      'nl',
                      'str',
                      'lmb',
                      'shdR',
                      'ankR',
                      'elbR',
                      'wstR',
                      't5R',
                      't3R',
                      'tail'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 8,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC88shuffle3.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'fliplr': False,
 'gaussian_blur': False,
 'gaussian_blur_params': {'sigma': [0.5, 4.0]},
 'global_scale': 0.8,
 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_88shuffle3.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]],
 'net_type': 'resnet_101',
 'num_joints': 16,
 'optimizer': 'adam',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset88shuffle3/train/snapshot',
 'stride': 8.0,
 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]],
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2022-07-31 12:01:03.186374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22344 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:67:00.0, compute capability: 8.6
Loading DLC 2.2.1.1...
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)
Selecting single-animal trainer
Batch Size is 8
Loading ImageNet-pretrained resnet_101
Training parameter:
{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset80shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'adam', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 8, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]], 'all_joints_names': ['t3L', 'wstL', 't5L', 'elbL', 'shdL', 'ankL', 'nl', 'str', 'lmb', 'shdR', 'ankR', 'elbR', 'wstR', 't5R', 't3R', 'tail'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC80shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_80shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]], 'net_type': 'resnet_101', 'num_joints': 16, 'pos_dist_thresh': 17, 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'gaussian_blur': False, 'gaussian_blur_params': {'sigma': [0.5, 4.0]}, 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]], 'fliplr': False, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}, 'multiply_and_add_to_brightness': False, 'snow': False, 'clouds': False, 'fog': False, 'snow_flakes': False, 'rain': False}
Starting training....
The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.
Selecting single-animal trainer
Batch Size is 8
Loading ImageNet-pretrained resnet_101
Training parameter:
{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset84shuffle2/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'adam', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 8, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]], 'all_joints_names': ['t3L', 'wstL', 't5L', 'elbL', 'shdL', 'ankL', 'nl', 'str', 'lmb', 'shdR', 'ankR', 'elbR', 'wstR', 't5R', 't3R', 'tail'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC84shuffle2.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_84shuffle2.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]], 'net_type': 'resnet_101', 'num_joints': 16, 'pos_dist_thresh': 17, 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'gaussian_blur': False, 'gaussian_blur_params': {'sigma': [0.5, 4.0]}, 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]], 'fliplr': False, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}, 'multiply_and_add_to_brightness': False, 'snow': False, 'clouds': False, 'fog': False, 'snow_flakes': False, 'rain': False}
Starting training....
The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.
Selecting single-animal trainer
Batch Size is 8
Loading ImageNet-pretrained resnet_101
Training parameter:
iteration: 1000 loss: 0.0282 lr: 0.0001
iteration: 2000 loss: 0.0147 lr: 0.0001
iteration: 3000 loss: 0.0125 lr: 0.0001
iteration: 4000 loss: 0.0109 lr: 0.0001
iteration: 5000 loss: 0.0102 lr: 0.0001
iteration: 6000 loss: 0.0094 lr: 0.0001
iteration: 7000 loss: 0.0088 lr: 0.0001
iteration: 8000 loss: 0.0081 lr: 5e-05
iteration: 9000 loss: 0.0070 lr: 5e-05
iteration: 10000 loss: 0.0069 lr: 5e-05
iteration: 11000 loss: 0.0067 lr: 5e-05
iteration: 12000 loss: 0.0064 lr: 5e-05
iteration: 13000 loss: 0.0057 lr: 1e-05
iteration: 14000 loss: 0.0054 lr: 1e-05
iteration: 15000 loss: 0.0053 lr: 1e-05
iteration: 16000 loss: 0.0053 lr: 1e-05
iteration: 17000 loss: 0.0051 lr: 1e-05
iteration: 18000 loss: 0.0050 lr: 1e-05
iteration: 19000 loss: 0.0050 lr: 1e-05
iteration: 20000 loss: 0.0048 lr: 1e-05
iteration: 21000 loss: 0.0048 lr: 1e-05
iteration: 22000 loss: 0.0048 lr: 1e-05
iteration: 23000 loss: 0.0049 lr: 1e-05
iteration: 24000 loss: 0.0046 lr: 1e-05
iteration: 25000 loss: 0.0046 lr: 1e-05
iteration: 26000 loss: 0.0046 lr: 1e-05
iteration: 27000 loss: 0.0046 lr: 1e-05
iteration: 28000 loss: 0.0046 lr: 1e-05
iteration: 29000 loss: 0.0045 lr: 1e-05
iteration: 30000 loss: 0.0045 lr: 1e-05
iteration: 31000 loss: 0.0044 lr: 1e-05
iteration: 32000 loss: 0.0043 lr: 1e-05
iteration: 33000 loss: 0.0044 lr: 1e-05
iteration: 34000 loss: 0.0043 lr: 1e-05
iteration: 35000 loss: 0.0043 lr: 1e-05
iteration: 36000 loss: 0.0042 lr: 1e-05
iteration: 37000 loss: 0.0043 lr: 1e-05
iteration: 38000 loss: 0.0042 lr: 1e-05
iteration: 39000 loss: 0.0042 lr: 1e-05
iteration: 40000 loss: 0.0042 lr: 1e-05
iteration: 41000 loss: 0.0042 lr: 1e-05
iteration: 42000 loss: 0.0040 lr: 1e-05
iteration: 43000 loss: 0.0041 lr: 1e-05
iteration: 44000 loss: 0.0040 lr: 1e-05
iteration: 45000 loss: 0.0041 lr: 1e-05
iteration: 46000 loss: 0.0040 lr: 1e-05
iteration: 47000 loss: 0.0039 lr: 1e-05
iteration: 48000 loss: 0.0038 lr: 1e-05
iteration: 49000 loss: 0.0040 lr: 1e-05
iteration: 50000 loss: 0.0040 lr: 1e-05
iteration: 51000 loss: 0.0038 lr: 1e-05
iteration: 52000 loss: 0.0039 lr: 1e-05
iteration: 53000 loss: 0.0038 lr: 1e-05
iteration: 54000 loss: 0.0038 lr: 1e-05
iteration: 55000 loss: 0.0038 lr: 1e-05
iteration: 56000 loss: 0.0038 lr: 1e-05
iteration: 57000 loss: 0.0038 lr: 1e-05
iteration: 58000 loss: 0.0037 lr: 1e-05
iteration: 59000 loss: 0.0037 lr: 1e-05
iteration: 60000 loss: 0.0036 lr: 1e-05
iteration: 61000 loss: 0.0037 lr: 1e-05
iteration: 62000 loss: 0.0037 lr: 1e-05
iteration: 63000 loss: 0.0036 lr: 1e-05
iteration: 64000 loss: 0.0037 lr: 1e-05
iteration: 65000 loss: 0.0036 lr: 1e-05
iteration: 66000 loss: 0.0036 lr: 1e-05
iteration: 67000 loss: 0.0036 lr: 1e-05
iteration: 68000 loss: 0.0036 lr: 1e-05
iteration: 69000 loss: 0.0035 lr: 1e-05
iteration: 70000 loss: 0.0036 lr: 1e-05
iteration: 71000 loss: 0.0035 lr: 1e-05
iteration: 72000 loss: 0.0036 lr: 1e-05
iteration: 73000 loss: 0.0035 lr: 1e-05
iteration: 74000 loss: 0.0035 lr: 1e-05
iteration: 75000 loss: 0.0035 lr: 1e-05
iteration: 76000 loss: 0.0034 lr: 1e-05
iteration: 77000 loss: 0.0035 lr: 1e-05
iteration: 78000 loss: 0.0034 lr: 1e-05
iteration: 79000 loss: 0.0034 lr: 1e-05
iteration: 80000 loss: 0.0034 lr: 1e-05
iteration: 81000 loss: 0.0034 lr: 1e-05
iteration: 82000 loss: 0.0034 lr: 1e-05
iteration: 83000 loss: 0.0034 lr: 1e-05
iteration: 84000 loss: 0.0034 lr: 1e-05
iteration: 85000 loss: 0.0033 lr: 1e-05
iteration: 86000 loss: 0.0033 lr: 1e-05
iteration: 87000 loss: 0.0034 lr: 1e-05
iteration: 88000 loss: 0.0034 lr: 1e-05
iteration: 89000 loss: 0.0033 lr: 1e-05
iteration: 90000 loss: 0.0033 lr: 1e-05
iteration: 91000 loss: 0.0033 lr: 1e-05
iteration: 92000 loss: 0.0033 lr: 1e-05
iteration: 93000 loss: 0.0032 lr: 1e-05
iteration: 94000 loss: 0.0032 lr: 1e-05
iteration: 95000 loss: 0.0033 lr: 1e-05
iteration: 96000 loss: 0.0032 lr: 1e-05
iteration: 97000 loss: 0.0032 lr: 1e-05
iteration: 98000 loss: 0.0031 lr: 1e-05
iteration: 99000 loss: 0.0033 lr: 1e-05
iteration: 100000 loss: 0.0032 lr: 1e-05
iteration: 101000 loss: 0.0031 lr: 1e-05
iteration: 102000 loss: 0.0031 lr: 1e-05
iteration: 103000 loss: 0.0032 lr: 1e-05
iteration: 104000 loss: 0.0031 lr: 1e-05
iteration: 105000 loss: 0.0031 lr: 1e-05
iteration: 106000 loss: 0.0032 lr: 1e-05
iteration: 107000 loss: 0.0032 lr: 1e-05
iteration: 108000 loss: 0.0031 lr: 1e-05
iteration: 109000 loss: 0.0032 lr: 1e-05
iteration: 110000 loss: 0.0031 lr: 1e-05
iteration: 111000 loss: 0.0031 lr: 1e-05
iteration: 112000 loss: 0.0031 lr: 1e-05
iteration: 113000 loss: 0.0031 lr: 1e-05
iteration: 114000 loss: 0.0031 lr: 1e-05
iteration: 115000 loss: 0.0031 lr: 1e-05
iteration: 116000 loss: 0.0031 lr: 1e-05
iteration: 117000 loss: 0.0031 lr: 1e-05
iteration: 118000 loss: 0.0030 lr: 1e-05
iteration: 119000 loss: 0.0030 lr: 1e-05
iteration: 120000 loss: 0.0030 lr: 1e-05
iteration: 121000 loss: 0.0031 lr: 1e-05
iteration: 122000 loss: 0.0030 lr: 1e-05
iteration: 123000 loss: 0.0030 lr: 1e-05
iteration: 124000 loss: 0.0030 lr: 1e-05
iteration: 125000 loss: 0.0030 lr: 1e-05
iteration: 126000 loss: 0.0030 lr: 1e-05
iteration: 127000 loss: 0.0029 lr: 1e-05
iteration: 128000 loss: 0.0030 lr: 1e-05
iteration: 129000 loss: 0.0030 lr: 1e-05
iteration: 130000 loss: 0.0029 lr: 1e-05
iteration: 131000 loss: 0.0029 lr: 1e-05
iteration: 132000 loss: 0.0029 lr: 1e-05
iteration: 133000 loss: 0.0029 lr: 1e-05
iteration: 134000 loss: 0.0029 lr: 1e-05
iteration: 135000 loss: 0.0030 lr: 1e-05
iteration: 136000 loss: 0.0029 lr: 1e-05
iteration: 137000 loss: 0.0029 lr: 1e-05
iteration: 138000 loss: 0.0029 lr: 1e-05
iteration: 139000 loss: 0.0029 lr: 1e-05
iteration: 140000 loss: 0.0029 lr: 1e-05
iteration: 141000 loss: 0.0029 lr: 1e-05
iteration: 142000 loss: 0.0030 lr: 1e-05
iteration: 143000 loss: 0.0028 lr: 1e-05
iteration: 144000 loss: 0.0029 lr: 1e-05
iteration: 145000 loss: 0.0029 lr: 1e-05
iteration: 146000 loss: 0.0029 lr: 1e-05
iteration: 147000 loss: 0.0029 lr: 1e-05
iteration: 148000 loss: 0.0029 lr: 1e-05
iteration: 149000 loss: 0.0029 lr: 1e-05
iteration: 150000 loss: 0.0028 lr: 1e-05
iteration: 151000 loss: 0.0028 lr: 1e-05
iteration: 152000 loss: 0.0028 lr: 1e-05
iteration: 153000 loss: 0.0028 lr: 1e-05
iteration: 154000 loss: 0.0028 lr: 1e-05
iteration: 155000 loss: 0.0029 lr: 1e-05
iteration: 156000 loss: 0.0029 lr: 1e-05
iteration: 157000 loss: 0.0028 lr: 1e-05
iteration: 158000 loss: 0.0028 lr: 1e-05
iteration: 159000 loss: 0.0027 lr: 1e-05
iteration: 160000 loss: 0.0028 lr: 1e-05
iteration: 161000 loss: 0.0027 lr: 1e-05
iteration: 162000 loss: 0.0028 lr: 1e-05
iteration: 163000 loss: 0.0027 lr: 1e-05
iteration: 164000 loss: 0.0027 lr: 1e-05
iteration: 165000 loss: 0.0028 lr: 1e-05
iteration: 166000 loss: 0.0027 lr: 1e-05
iteration: 167000 loss: 0.0028 lr: 1e-05
iteration: 168000 loss: 0.0027 lr: 1e-05
iteration: 169000 loss: 0.0027 lr: 1e-05
iteration: 170000 loss: 0.0029 lr: 1e-05
iteration: 171000 loss: 0.0027 lr: 1e-05
iteration: 172000 loss: 0.0027 lr: 1e-05
iteration: 173000 loss: 0.0027 lr: 1e-05
iteration: 174000 loss: 0.0027 lr: 1e-05
iteration: 175000 loss: 0.0027 lr: 1e-05
iteration: 176000 loss: 0.0027 lr: 1e-05
iteration: 177000 loss: 0.0026 lr: 1e-05
iteration: 178000 loss: 0.0026 lr: 1e-05
iteration: 179000 loss: 0.0027 lr: 1e-05
iteration: 180000 loss: 0.0027 lr: 1e-05
iteration: 181000 loss: 0.0027 lr: 1e-05
iteration: 182000 loss: 0.0027 lr: 1e-05
iteration: 183000 loss: 0.0026 lr: 1e-05
iteration: 184000 loss: 0.0026 lr: 1e-05
iteration: 185000 loss: 0.0027 lr: 1e-05
iteration: 186000 loss: 0.0027 lr: 1e-05
iteration: 187000 loss: 0.0026 lr: 1e-05
iteration: 188000 loss: 0.0027 lr: 1e-05
iteration: 189000 loss: 0.0026 lr: 1e-05
iteration: 190000 loss: 0.0026 lr: 1e-05
iteration: 191000 loss: 0.0026 lr: 1e-05
iteration: 192000 loss: 0.0028 lr: 1e-05
iteration: 193000 loss: 0.0026 lr: 1e-05
iteration: 194000 loss: 0.0027 lr: 1e-05
iteration: 195000 loss: 0.0026 lr: 1e-05
iteration: 196000 loss: 0.0026 lr: 1e-05
iteration: 197000 loss: 0.0027 lr: 1e-05
iteration: 198000 loss: 0.0027 lr: 1e-05
iteration: 199000 loss: 0.0026 lr: 1e-05
iteration: 200000 loss: 0.0026 lr: 1e-05
iteration: 201000 loss: 0.0026 lr: 1e-05
iteration: 202000 loss: 0.0026 lr: 1e-05
iteration: 203000 loss: 0.0026 lr: 1e-05
iteration: 204000 loss: 0.0026 lr: 1e-05
iteration: 205000 loss: 0.0026 lr: 1e-05
iteration: 206000 loss: 0.0027 lr: 1e-05
iteration: 207000 loss: 0.0026 lr: 1e-05
iteration: 208000 loss: 0.0025 lr: 1e-05
iteration: 209000 loss: 0.0026 lr: 1e-05
iteration: 210000 loss: 0.0026 lr: 1e-05
iteration: 211000 loss: 0.0026 lr: 1e-05
iteration: 212000 loss: 0.0026 lr: 1e-05
iteration: 213000 loss: 0.0027 lr: 1e-05
iteration: 214000 loss: 0.0025 lr: 1e-05
iteration: 215000 loss: 0.0026 lr: 1e-05
iteration: 216000 loss: 0.0025 lr: 1e-05
iteration: 217000 loss: 0.0025 lr: 1e-05
iteration: 218000 loss: 0.0025 lr: 1e-05
iteration: 219000 loss: 0.0026 lr: 1e-05
iteration: 220000 loss: 0.0025 lr: 1e-05
iteration: 221000 loss: 0.0025 lr: 1e-05
iteration: 222000 loss: 0.0026 lr: 1e-05
iteration: 223000 loss: 0.0025 lr: 1e-05
iteration: 224000 loss: 0.0025 lr: 1e-05
iteration: 225000 loss: 0.0025 lr: 1e-05
iteration: 226000 loss: 0.0025 lr: 1e-05
iteration: 227000 loss: 0.0025 lr: 1e-05
iteration: 228000 loss: 0.0025 lr: 1e-05
iteration: 229000 loss: 0.0025 lr: 1e-05
iteration: 230000 loss: 0.0025 lr: 1e-05
iteration: 231000 loss: 0.0025 lr: 1e-05
iteration: 232000 loss: 0.0025 lr: 1e-05
iteration: 233000 loss: 0.0024 lr: 1e-05
iteration: 234000 loss: 0.0025 lr: 1e-05
iteration: 235000 loss: 0.0025 lr: 1e-05
iteration: 236000 loss: 0.0024 lr: 1e-05
iteration: 237000 loss: 0.0025 lr: 1e-05
iteration: 238000 loss: 0.0024 lr: 1e-05
iteration: 239000 loss: 0.0025 lr: 1e-05
iteration: 240000 loss: 0.0024 lr: 1e-05
iteration: 241000 loss: 0.0025 lr: 1e-05
iteration: 242000 loss: 0.0025 lr: 1e-05
iteration: 243000 loss: 0.0025 lr: 1e-05
iteration: 244000 loss: 0.0025 lr: 1e-05
iteration: 245000 loss: 0.0025 lr: 1e-05
iteration: 246000 loss: 0.0024 lr: 1e-05
iteration: 247000 loss: 0.0024 lr: 1e-05
iteration: 248000 loss: 0.0025 lr: 1e-05
iteration: 249000 loss: 0.0024 lr: 1e-05
iteration: 250000 loss: 0.0025 lr: 1e-05
iteration: 251000 loss: 0.0024 lr: 1e-05
iteration: 252000 loss: 0.0024 lr: 1e-05
iteration: 253000 loss: 0.0024 lr: 1e-05
iteration: 254000 loss: 0.0025 lr: 1e-05
iteration: 255000 loss: 0.0024 lr: 1e-05
iteration: 256000 loss: 0.0025 lr: 1e-05
iteration: 257000 loss: 0.0024 lr: 1e-05
iteration: 258000 loss: 0.0025 lr: 1e-05
iteration: 259000 loss: 0.0024 lr: 1e-05
iteration: 260000 loss: 0.0025 lr: 1e-05
iteration: 261000 loss: 0.0024 lr: 1e-05
iteration: 262000 loss: 0.0025 lr: 1e-05
iteration: 263000 loss: 0.0024 lr: 1e-05
iteration: 264000 loss: 0.0024 lr: 1e-05
iteration: 265000 loss: 0.0025 lr: 1e-05
iteration: 266000 loss: 0.0024 lr: 1e-05
iteration: 267000 loss: 0.0024 lr: 1e-05
iteration: 268000 loss: 0.0024 lr: 1e-05
iteration: 269000 loss: 0.0024 lr: 1e-05
iteration: 270000 loss: 0.0024 lr: 1e-05
iteration: 271000 loss: 0.0024 lr: 1e-05
iteration: 272000 loss: 0.0024 lr: 1e-05
iteration: 273000 loss: 0.0024 lr: 1e-05
iteration: 274000 loss: 0.0024 lr: 1e-05
iteration: 275000 loss: 0.0024 lr: 1e-05
iteration: 276000 loss: 0.0023 lr: 1e-05
iteration: 277000 loss: 0.0024 lr: 1e-05
iteration: 278000 loss: 0.0024 lr: 1e-05
iteration: 279000 loss: 0.0024 lr: 1e-05
iteration: 280000 loss: 0.0024 lr: 1e-05
iteration: 281000 loss: 0.0024 lr: 1e-05
iteration: 282000 loss: 0.0023 lr: 1e-05
iteration: 283000 loss: 0.0023 lr: 1e-05
iteration: 284000 loss: 0.0024 lr: 1e-05
iteration: 285000 loss: 0.0024 lr: 1e-05
iteration: 286000 loss: 0.0023 lr: 1e-05
iteration: 287000 loss: 0.0023 lr: 1e-05
iteration: 288000 loss: 0.0023 lr: 1e-05
iteration: 289000 loss: 0.0023 lr: 1e-05
iteration: 290000 loss: 0.0023 lr: 1e-05
iteration: 291000 loss: 0.0024 lr: 1e-05
iteration: 292000 loss: 0.0023 lr: 1e-05
iteration: 293000 loss: 0.0023 lr: 1e-05
iteration: 294000 loss: 0.0023 lr: 1e-05
iteration: 295000 loss: 0.0024 lr: 1e-05
iteration: 296000 loss: 0.0023 lr: 1e-05
iteration: 297000 loss: 0.0024 lr: 1e-05
iteration: 298000 loss: 0.0023 lr: 1e-05
iteration: 299000 loss: 0.0023 lr: 1e-05
iteration: 300000 loss: 0.0023 lr: 1e-05
iteration: 301000 loss: 0.0023 lr: 1e-05
iteration: 302000 loss: 0.0023 lr: 1e-05
iteration: 303000 loss: 0.0023 lr: 1e-05
iteration: 304000 loss: 0.0023 lr: 1e-05
iteration: 305000 loss: 0.0023 lr: 1e-05
iteration: 306000 loss: 0.0023 lr: 1e-05
iteration: 307000 loss: 0.0024 lr: 1e-05
iteration: 308000 loss: 0.0023 lr: 1e-05
iteration: 309000 loss: 0.0023 lr: 1e-05
iteration: 310000 loss: 0.0023 lr: 1e-05
iteration: 311000 loss: 0.0024 lr: 1e-05
iteration: 312000 loss: 0.0024 lr: 1e-05
iteration: 313000 loss: 0.0023 lr: 1e-05
iteration: 314000 loss: 0.0023 lr: 1e-05
iteration: 315000 loss: 0.0023 lr: 1e-05
iteration: 316000 loss: 0.0023 lr: 1e-05
iteration: 317000 loss: 0.0023 lr: 1e-05
iteration: 318000 loss: 0.0023 lr: 1e-05
iteration: 319000 loss: 0.0023 lr: 1e-05
iteration: 320000 loss: 0.0023 lr: 1e-05
iteration: 321000 loss: 0.0023 lr: 1e-05
iteration: 322000 loss: 0.0022 lr: 1e-05
iteration: 323000 loss: 0.0023 lr: 1e-05
iteration: 324000 loss: 0.0022 lr: 1e-05
iteration: 325000 loss: 0.0023 lr: 1e-05
iteration: 326000 loss: 0.0023 lr: 1e-05
iteration: 327000 loss: 0.0023 lr: 1e-05
iteration: 328000 loss: 0.0022 lr: 1e-05
iteration: 329000 loss: 0.0022 lr: 1e-05
iteration: 330000 loss: 0.0022 lr: 1e-05
iteration: 331000 loss: 0.0022 lr: 1e-05
iteration: 332000 loss: 0.0023 lr: 1e-05
iteration: 333000 loss: 0.0022 lr: 1e-05
iteration: 334000 loss: 0.0023 lr: 1e-05
iteration: 335000 loss: 0.0023 lr: 1e-05
iteration: 336000 loss: 0.0022 lr: 1e-05
iteration: 337000 loss: 0.0023 lr: 1e-05
iteration: 338000 loss: 0.0022 lr: 1e-05
iteration: 339000 loss: 0.0023 lr: 1e-05
iteration: 340000 loss: 0.0022 lr: 1e-05
iteration: 341000 loss: 0.0023 lr: 1e-05
iteration: 342000 loss: 0.0022 lr: 1e-05
iteration: 343000 loss: 0.0023 lr: 1e-05
iteration: 344000 loss: 0.0022 lr: 1e-05
iteration: 345000 loss: 0.0022 lr: 1e-05
iteration: 346000 loss: 0.0022 lr: 1e-05
iteration: 347000 loss: 0.0022 lr: 1e-05
iteration: 348000 loss: 0.0023 lr: 1e-05
iteration: 349000 loss: 0.0022 lr: 1e-05
iteration: 350000 loss: 0.0023 lr: 1e-05
iteration: 351000 loss: 0.0023 lr: 1e-05
iteration: 352000 loss: 0.0022 lr: 1e-05
iteration: 353000 loss: 0.0022 lr: 1e-05
iteration: 354000 loss: 0.0022 lr: 1e-05
iteration: 355000 loss: 0.0022 lr: 1e-05
iteration: 356000 loss: 0.0022 lr: 1e-05
iteration: 357000 loss: 0.0022 lr: 1e-05
iteration: 358000 loss: 0.0022 lr: 1e-05
iteration: 359000 loss: 0.0022 lr: 1e-05
iteration: 360000 loss: 0.0022 lr: 1e-05
iteration: 361000 loss: 0.0022 lr: 1e-05
iteration: 362000 loss: 0.0022 lr: 1e-05
iteration: 363000 loss: 0.0023 lr: 1e-05
iteration: 364000 loss: 0.0022 lr: 1e-05
iteration: 365000 loss: 0.0022 lr: 1e-05
iteration: 366000 loss: 0.0022 lr: 1e-05
iteration: 367000 loss: 0.0022 lr: 1e-05
iteration: 368000 loss: 0.0022 lr: 1e-05
iteration: 369000 loss: 0.0022 lr: 1e-05
iteration: 370000 loss: 0.0022 lr: 1e-05
iteration: 371000 loss: 0.0022 lr: 1e-05
iteration: 372000 loss: 0.0021 lr: 1e-05
iteration: 373000 loss: 0.0022 lr: 1e-05
iteration: 374000 loss: 0.0021 lr: 1e-05
iteration: 375000 loss: 0.0022 lr: 1e-05
iteration: 376000 loss: 0.0021 lr: 1e-05
iteration: 377000 loss: 0.0022 lr: 1e-05
iteration: 378000 loss: 0.0022 lr: 1e-05
iteration: 379000 loss: 0.0022 lr: 1e-05
iteration: 380000 loss: 0.0022 lr: 1e-05
iteration: 381000 loss: 0.0021 lr: 1e-05
iteration: 382000 loss: 0.0021 lr: 1e-05
iteration: 383000 loss: 0.0022 lr: 1e-05
iteration: 384000 loss: 0.0022 lr: 1e-05
iteration: 385000 loss: 0.0022 lr: 1e-05
iteration: 386000 loss: 0.0022 lr: 1e-05
iteration: 387000 loss: 0.0022 lr: 1e-05
iteration: 388000 loss: 0.0022 lr: 1e-05
iteration: 389000 loss: 0.0021 lr: 1e-05
iteration: 390000 loss: 0.0021 lr: 1e-05
iteration: 391000 loss: 0.0022 lr: 1e-05
iteration: 392000 loss: 0.0021 lr: 1e-05
iteration: 393000 loss: 0.0022 lr: 1e-05
iteration: 394000 loss: 0.0022 lr: 1e-05
iteration: 395000 loss: 0.0022 lr: 1e-05
iteration: 396000 loss: 0.0022 lr: 1e-05
iteration: 397000 loss: 0.0022 lr: 1e-05
iteration: 398000 loss: 0.0022 lr: 1e-05
iteration: 399000 loss: 0.0022 lr: 1e-05
iteration: 400000 loss: 0.0021 lr: 1e-05
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1377, in _do_call
    return fn(*args)
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1360, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1453, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/core/train.py", line 83, in load_and_enqueue
    sess.run(enqueue_op, feed_dict=food)
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1370, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1396, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.CancelledError: Graph execution error:

Run call was cancelled
Config:
{'all_joints': [[0],
                [1],
                [2],
                [3],
                [4],
                [5],
                [6],
                [7],
                [8],
                [9],
                [10],
                [11],
                [12],
                [13],
                [14],
                [15]],
 'all_joints_names': ['t3L',
                      'wstL',
                      't5L',
                      'elbL',
                      'shdL',
                      'ankL',
                      'nl',
                      'str',
                      'lmb',
                      'shdR',
                      'ankR',
                      'elbR',
                      'wstR',
                      't5R',
                      't3R',
                      'tail'],
 'alpha_r': 0.02,
 'apply_prob': 0.5,
 'batch_size': 8,
 'contrast': {'clahe': True,
              'claheratio': 0.1,
              'histeq': True,
              'histeqratio': 0.1},
 'convolution': {'edge': False,
                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},
                 'embossratio': 0.1,
                 'sharpen': False,
                 'sharpenratio': 0.3},
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC89shuffle4.mat',
 'dataset_type': 'imgaug',
 'decay_steps': 30000,
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'fliplr': False,
 'gaussian_blur': False,
 'gaussian_blur_params': {'sigma': [0.5, 4.0]},
 'global_scale': 0.8,
 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'lr_init': 0.0005,
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_89shuffle4.pickle',
 'min_input_size': 64,
 'mirror': False,
 'multi_stage': False,
 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]],
 'net_type': 'resnet_101',
 'num_joints': 16,
 'optimizer': 'adam',
 'pairwise_huber_loss': False,
 'pairwise_predict': False,
 'partaffinityfield_predict': False,
 'pos_dist_thresh': 17,
 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29',
 'regularize': False,
 'rotation': 25,
 'rotratio': 0.4,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset89shuffle4/train/snapshot',
 'stride': 8.0,
 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]],
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2022-08-01 09:21:02.364846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22344 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:67:00.0, compute capability: 8.6
iteration: 1000 loss: 0.0282 lr: 0.0001
iteration: 2000 loss: 0.0153 lr: 0.0001
iteration: 3000 loss: 0.0124 lr: 0.0001
iteration: 4000 loss: 0.0111 lr: 0.0001
iteration: 5000 loss: 0.0104 lr: 0.0001
iteration: 6000 loss: 0.0096 lr: 0.0001
iteration: 7000 loss: 0.0092 lr: 0.0001
iteration: 8000 loss: 0.0083 lr: 5e-05
iteration: 9000 loss: 0.0073 lr: 5e-05
iteration: 10000 loss: 0.0069 lr: 5e-05
iteration: 11000 loss: 0.0071 lr: 5e-05
iteration: 12000 loss: 0.0067 lr: 5e-05
iteration: 13000 loss: 0.0059 lr: 1e-05
iteration: 14000 loss: 0.0056 lr: 1e-05
iteration: 15000 loss: 0.0055 lr: 1e-05
iteration: 16000 loss: 0.0055 lr: 1e-05
iteration: 17000 loss: 0.0053 lr: 1e-05
iteration: 18000 loss: 0.0054 lr: 1e-05
iteration: 19000 loss: 0.0053 lr: 1e-05
iteration: 20000 loss: 0.0052 lr: 1e-05
iteration: 21000 loss: 0.0051 lr: 1e-05
iteration: 22000 loss: 0.0050 lr: 1e-05
iteration: 23000 loss: 0.0050 lr: 1e-05
iteration: 24000 loss: 0.0049 lr: 1e-05
iteration: 25000 loss: 0.0050 lr: 1e-05
iteration: 26000 loss: 0.0049 lr: 1e-05
iteration: 27000 loss: 0.0047 lr: 1e-05
iteration: 28000 loss: 0.0048 lr: 1e-05
iteration: 29000 loss: 0.0047 lr: 1e-05
iteration: 30000 loss: 0.0047 lr: 1e-05
iteration: 31000 loss: 0.0046 lr: 1e-05
iteration: 32000 loss: 0.0046 lr: 1e-05
iteration: 33000 loss: 0.0046 lr: 1e-05
iteration: 34000 loss: 0.0046 lr: 1e-05
iteration: 35000 loss: 0.0045 lr: 1e-05
iteration: 36000 loss: 0.0045 lr: 1e-05
iteration: 37000 loss: 0.0045 lr: 1e-05
iteration: 38000 loss: 0.0045 lr: 1e-05
iteration: 39000 loss: 0.0045 lr: 1e-05
iteration: 40000 loss: 0.0043 lr: 1e-05
iteration: 41000 loss: 0.0045 lr: 1e-05
iteration: 42000 loss: 0.0043 lr: 1e-05
iteration: 43000 loss: 0.0042 lr: 1e-05
iteration: 44000 loss: 0.0041 lr: 1e-05
iteration: 45000 loss: 0.0042 lr: 1e-05
iteration: 46000 loss: 0.0042 lr: 1e-05
iteration: 47000 loss: 0.0042 lr: 1e-05
iteration: 48000 loss: 0.0042 lr: 1e-05
iteration: 49000 loss: 0.0043 lr: 1e-05
iteration: 50000 loss: 0.0041 lr: 1e-05
iteration: 51000 loss: 0.0041 lr: 1e-05
iteration: 52000 loss: 0.0040 lr: 1e-05
iteration: 53000 loss: 0.0041 lr: 1e-05
iteration: 54000 loss: 0.0040 lr: 1e-05
iteration: 55000 loss: 0.0040 lr: 1e-05
iteration: 56000 loss: 0.0041 lr: 1e-05
iteration: 57000 loss: 0.0040 lr: 1e-05
iteration: 58000 loss: 0.0040 lr: 1e-05
iteration: 59000 loss: 0.0039 lr: 1e-05
iteration: 60000 loss: 0.0038 lr: 1e-05
iteration: 61000 loss: 0.0038 lr: 1e-05
iteration: 62000 loss: 0.0038 lr: 1e-05
iteration: 63000 loss: 0.0039 lr: 1e-05
iteration: 64000 loss: 0.0038 lr: 1e-05
iteration: 65000 loss: 0.0038 lr: 1e-05
iteration: 66000 loss: 0.0038 lr: 1e-05
iteration: 67000 loss: 0.0037 lr: 1e-05
iteration: 68000 loss: 0.0037 lr: 1e-05
iteration: 69000 loss: 0.0037 lr: 1e-05
iteration: 70000 loss: 0.0037 lr: 1e-05
iteration: 71000 loss: 0.0038 lr: 1e-05
iteration: 72000 loss: 0.0036 lr: 1e-05
iteration: 73000 loss: 0.0036 lr: 1e-05
iteration: 74000 loss: 0.0036 lr: 1e-05
iteration: 75000 loss: 0.0037 lr: 1e-05
iteration: 76000 loss: 0.0036 lr: 1e-05
iteration: 77000 loss: 0.0036 lr: 1e-05
iteration: 78000 loss: 0.0036 lr: 1e-05
iteration: 79000 loss: 0.0035 lr: 1e-05
iteration: 80000 loss: 0.0035 lr: 1e-05
iteration: 81000 loss: 0.0035 lr: 1e-05
iteration: 82000 loss: 0.0035 lr: 1e-05
iteration: 83000 loss: 0.0035 lr: 1e-05
iteration: 84000 loss: 0.0035 lr: 1e-05
iteration: 85000 loss: 0.0035 lr: 1e-05
iteration: 86000 loss: 0.0035 lr: 1e-05
iteration: 87000 loss: 0.0035 lr: 1e-05
iteration: 88000 loss: 0.0034 lr: 1e-05
iteration: 89000 loss: 0.0034 lr: 1e-05
iteration: 90000 loss: 0.0035 lr: 1e-05
iteration: 91000 loss: 0.0034 lr: 1e-05
iteration: 92000 loss: 0.0034 lr: 1e-05
iteration: 93000 loss: 0.0034 lr: 1e-05
iteration: 94000 loss: 0.0033 lr: 1e-05
iteration: 95000 loss: 0.0034 lr: 1e-05
iteration: 96000 loss: 0.0034 lr: 1e-05
iteration: 97000 loss: 0.0034 lr: 1e-05
iteration: 98000 loss: 0.0034 lr: 1e-05
iteration: 99000 loss: 0.0033 lr: 1e-05
iteration: 100000 loss: 0.0032 lr: 1e-05
iteration: 101000 loss: 0.0034 lr: 1e-05
iteration: 102000 loss: 0.0033 lr: 1e-05
iteration: 103000 loss: 0.0033 lr: 1e-05
iteration: 104000 loss: 0.0033 lr: 1e-05
iteration: 105000 loss: 0.0034 lr: 1e-05
iteration: 106000 loss: 0.0032 lr: 1e-05
iteration: 107000 loss: 0.0032 lr: 1e-05
iteration: 108000 loss: 0.0031 lr: 1e-05
iteration: 109000 loss: 0.0032 lr: 1e-05
iteration: 110000 loss: 0.0033 lr: 1e-05
iteration: 111000 loss: 0.0032 lr: 1e-05
iteration: 112000 loss: 0.0032 lr: 1e-05
iteration: 113000 loss: 0.0032 lr: 1e-05
iteration: 114000 loss: 0.0032 lr: 1e-05
iteration: 115000 loss: 0.0032 lr: 1e-05
iteration: 116000 loss: 0.0031 lr: 1e-05
iteration: 117000 loss: 0.0032 lr: 1e-05
iteration: 118000 loss: 0.0031 lr: 1e-05
iteration: 119000 loss: 0.0031 lr: 1e-05
iteration: 120000 loss: 0.0031 lr: 1e-05
iteration: 121000 loss: 0.0031 lr: 1e-05
iteration: 122000 loss: 0.0031 lr: 1e-05
iteration: 123000 loss: 0.0031 lr: 1e-05
iteration: 124000 loss: 0.0032 lr: 1e-05
iteration: 125000 loss: 0.0031 lr: 1e-05
iteration: 126000 loss: 0.0030 lr: 1e-05
iteration: 127000 loss: 0.0031 lr: 1e-05
iteration: 128000 loss: 0.0030 lr: 1e-05
iteration: 129000 loss: 0.0031 lr: 1e-05
iteration: 130000 loss: 0.0031 lr: 1e-05
iteration: 131000 loss: 0.0031 lr: 1e-05
iteration: 132000 loss: 0.0030 lr: 1e-05
iteration: 133000 loss: 0.0031 lr: 1e-05
iteration: 134000 loss: 0.0030 lr: 1e-05
iteration: 135000 loss: 0.0030 lr: 1e-05
iteration: 136000 loss: 0.0031 lr: 1e-05
iteration: 137000 loss: 0.0029 lr: 1e-05
iteration: 138000 loss: 0.0030 lr: 1e-05
iteration: 139000 loss: 0.0030 lr: 1e-05
iteration: 140000 loss: 0.0030 lr: 1e-05
iteration: 141000 loss: 0.0030 lr: 1e-05
iteration: 142000 loss: 0.0030 lr: 1e-05
iteration: 143000 loss: 0.0029 lr: 1e-05
iteration: 144000 loss: 0.0030 lr: 1e-05
iteration: 145000 loss: 0.0030 lr: 1e-05
iteration: 146000 loss: 0.0030 lr: 1e-05
iteration: 147000 loss: 0.0030 lr: 1e-05
iteration: 148000 loss: 0.0029 lr: 1e-05
iteration: 149000 loss: 0.0029 lr: 1e-05
iteration: 150000 loss: 0.0030 lr: 1e-05
iteration: 151000 loss: 0.0028 lr: 1e-05
iteration: 152000 loss: 0.0029 lr: 1e-05
iteration: 153000 loss: 0.0029 lr: 1e-05
iteration: 154000 loss: 0.0029 lr: 1e-05
iteration: 155000 loss: 0.0029 lr: 1e-05
iteration: 156000 loss: 0.0029 lr: 1e-05
iteration: 157000 loss: 0.0029 lr: 1e-05
iteration: 158000 loss: 0.0029 lr: 1e-05
iteration: 159000 loss: 0.0029 lr: 1e-05
iteration: 160000 loss: 0.0028 lr: 1e-05
iteration: 161000 loss: 0.0029 lr: 1e-05
iteration: 162000 loss: 0.0029 lr: 1e-05
iteration: 163000 loss: 0.0029 lr: 1e-05
iteration: 164000 loss: 0.0029 lr: 1e-05
iteration: 165000 loss: 0.0028 lr: 1e-05
iteration: 166000 loss: 0.0028 lr: 1e-05
iteration: 167000 loss: 0.0028 lr: 1e-05
iteration: 168000 loss: 0.0029 lr: 1e-05
iteration: 169000 loss: 0.0028 lr: 1e-05
iteration: 170000 loss: 0.0029 lr: 1e-05
iteration: 171000 loss: 0.0028 lr: 1e-05
iteration: 172000 loss: 0.0028 lr: 1e-05
iteration: 173000 loss: 0.0028 lr: 1e-05
iteration: 174000 loss: 0.0028 lr: 1e-05
iteration: 175000 loss: 0.0028 lr: 1e-05
iteration: 176000 loss: 0.0028 lr: 1e-05
iteration: 177000 loss: 0.0027 lr: 1e-05
iteration: 178000 loss: 0.0027 lr: 1e-05
iteration: 179000 loss: 0.0027 lr: 1e-05
iteration: 180000 loss: 0.0028 lr: 1e-05
iteration: 181000 loss: 0.0028 lr: 1e-05
iteration: 182000 loss: 0.0028 lr: 1e-05
iteration: 183000 loss: 0.0028 lr: 1e-05
iteration: 184000 loss: 0.0028 lr: 1e-05
iteration: 185000 loss: 0.0027 lr: 1e-05
iteration: 186000 loss: 0.0027 lr: 1e-05
iteration: 187000 loss: 0.0027 lr: 1e-05
iteration: 188000 loss: 0.0027 lr: 1e-05
iteration: 189000 loss: 0.0027 lr: 1e-05
iteration: 190000 loss: 0.0027 lr: 1e-05
iteration: 191000 loss: 0.0027 lr: 1e-05
iteration: 192000 loss: 0.0027 lr: 1e-05
iteration: 193000 loss: 0.0028 lr: 1e-05
iteration: 194000 loss: 0.0028 lr: 1e-05
iteration: 195000 loss: 0.0027 lr: 1e-05
iteration: 196000 loss: 0.0027 lr: 1e-05
iteration: 197000 loss: 0.0028 lr: 1e-05
iteration: 198000 loss: 0.0027 lr: 1e-05
iteration: 199000 loss: 0.0027 lr: 1e-05
iteration: 200000 loss: 0.0027 lr: 1e-05
iteration: 201000 loss: 0.0027 lr: 1e-05
iteration: 202000 loss: 0.0027 lr: 1e-05
iteration: 203000 loss: 0.0027 lr: 1e-05
iteration: 204000 loss: 0.0027 lr: 1e-05
iteration: 205000 loss: 0.0027 lr: 1e-05
iteration: 206000 loss: 0.0027 lr: 1e-05
iteration: 207000 loss: 0.0027 lr: 1e-05
iteration: 208000 loss: 0.0026 lr: 1e-05
iteration: 209000 loss: 0.0026 lr: 1e-05
iteration: 210000 loss: 0.0026 lr: 1e-05
iteration: 211000 loss: 0.0027 lr: 1e-05
iteration: 212000 loss: 0.0027 lr: 1e-05
iteration: 213000 loss: 0.0026 lr: 1e-05
iteration: 214000 loss: 0.0026 lr: 1e-05
iteration: 215000 loss: 0.0026 lr: 1e-05
iteration: 216000 loss: 0.0026 lr: 1e-05
iteration: 217000 loss: 0.0025 lr: 1e-05
iteration: 218000 loss: 0.0026 lr: 1e-05
iteration: 219000 loss: 0.0026 lr: 1e-05
iteration: 220000 loss: 0.0026 lr: 1e-05
iteration: 221000 loss: 0.0026 lr: 1e-05
iteration: 222000 loss: 0.0026 lr: 1e-05
iteration: 223000 loss: 0.0025 lr: 1e-05
iteration: 224000 loss: 0.0026 lr: 1e-05
iteration: 225000 loss: 0.0026 lr: 1e-05
iteration: 226000 loss: 0.0025 lr: 1e-05
iteration: 227000 loss: 0.0026 lr: 1e-05
iteration: 228000 loss: 0.0026 lr: 1e-05
iteration: 229000 loss: 0.0025 lr: 1e-05
iteration: 230000 loss: 0.0025 lr: 1e-05
iteration: 231000 loss: 0.0026 lr: 1e-05
iteration: 232000 loss: 0.0026 lr: 1e-05
iteration: 233000 loss: 0.0027 lr: 1e-05
iteration: 234000 loss: 0.0026 lr: 1e-05
iteration: 235000 loss: 0.0025 lr: 1e-05
iteration: 236000 loss: 0.0025 lr: 1e-05
iteration: 237000 loss: 0.0026 lr: 1e-05
iteration: 238000 loss: 0.0026 lr: 1e-05
iteration: 239000 loss: 0.0025 lr: 1e-05
iteration: 240000 loss: 0.0025 lr: 1e-05
iteration: 241000 loss: 0.0025 lr: 1e-05
iteration: 242000 loss: 0.0025 lr: 1e-05
iteration: 243000 loss: 0.0026 lr: 1e-05
iteration: 244000 loss: 0.0025 lr: 1e-05
iteration: 245000 loss: 0.0025 lr: 1e-05
iteration: 246000 loss: 0.0026 lr: 1e-05
iteration: 247000 loss: 0.0025 lr: 1e-05
iteration: 248000 loss: 0.0026 lr: 1e-05
iteration: 249000 loss: 0.0025 lr: 1e-05
iteration: 250000 loss: 0.0025 lr: 1e-05
iteration: 251000 loss: 0.0025 lr: 1e-05
iteration: 252000 loss: 0.0025 lr: 1e-05
iteration: 253000 loss: 0.0025 lr: 1e-05
iteration: 254000 loss: 0.0025 lr: 1e-05
iteration: 255000 loss: 0.0025 lr: 1e-05
iteration: 256000 loss: 0.0025 lr: 1e-05
iteration: 257000 loss: 0.0025 lr: 1e-05
iteration: 258000 loss: 0.0025 lr: 1e-05
iteration: 259000 loss: 0.0025 lr: 1e-05
iteration: 260000 loss: 0.0024 lr: 1e-05
iteration: 261000 loss: 0.0024 lr: 1e-05
iteration: 262000 loss: 0.0025 lr: 1e-05
iteration: 263000 loss: 0.0024 lr: 1e-05
iteration: 264000 loss: 0.0025 lr: 1e-05
iteration: 265000 loss: 0.0025 lr: 1e-05
iteration: 266000 loss: 0.0024 lr: 1e-05
iteration: 267000 loss: 0.0024 lr: 1e-05
iteration: 268000 loss: 0.0025 lr: 1e-05
iteration: 269000 loss: 0.0025 lr: 1e-05
iteration: 270000 loss: 0.0024 lr: 1e-05
iteration: 271000 loss: 0.0024 lr: 1e-05
iteration: 272000 loss: 0.0024 lr: 1e-05
iteration: 273000 loss: 0.0024 lr: 1e-05
iteration: 274000 loss: 0.0025 lr: 1e-05
iteration: 275000 loss: 0.0024 lr: 1e-05
iteration: 276000 loss: 0.0024 lr: 1e-05
iteration: 277000 loss: 0.0024 lr: 1e-05
iteration: 278000 loss: 0.0025 lr: 1e-05
iteration: 279000 loss: 0.0025 lr: 1e-05
iteration: 280000 loss: 0.0024 lr: 1e-05
iteration: 281000 loss: 0.0024 lr: 1e-05
iteration: 282000 loss: 0.0025 lr: 1e-05
iteration: 283000 loss: 0.0024 lr: 1e-05
iteration: 284000 loss: 0.0024 lr: 1e-05
iteration: 285000 loss: 0.0024 lr: 1e-05
iteration: 286000 loss: 0.0024 lr: 1e-05
iteration: 287000 loss: 0.0024 lr: 1e-05
iteration: 288000 loss: 0.0024 lr: 1e-05
iteration: 289000 loss: 0.0024 lr: 1e-05
iteration: 290000 loss: 0.0024 lr: 1e-05
iteration: 291000 loss: 0.0025 lr: 1e-05
iteration: 292000 loss: 0.0024 lr: 1e-05
iteration: 293000 loss: 0.0024 lr: 1e-05
iteration: 294000 loss: 0.0024 lr: 1e-05
iteration: 295000 loss: 0.0024 lr: 1e-05
iteration: 296000 loss: 0.0024 lr: 1e-05
iteration: 297000 loss: 0.0024 lr: 1e-05
iteration: 298000 loss: 0.0024 lr: 1e-05
iteration: 299000 loss: 0.0024 lr: 1e-05
iteration: 300000 loss: 0.0024 lr: 1e-05
iteration: 301000 loss: 0.0024 lr: 1e-05
iteration: 302000 loss: 0.0024 lr: 1e-05
iteration: 303000 loss: 0.0024 lr: 1e-05
iteration: 304000 loss: 0.0024 lr: 1e-05
iteration: 305000 loss: 0.0024 lr: 1e-05
iteration: 306000 loss: 0.0023 lr: 1e-05
iteration: 307000 loss: 0.0023 lr: 1e-05
iteration: 308000 loss: 0.0024 lr: 1e-05
iteration: 309000 loss: 0.0024 lr: 1e-05
iteration: 310000 loss: 0.0024 lr: 1e-05
iteration: 311000 loss: 0.0024 lr: 1e-05
iteration: 312000 loss: 0.0024 lr: 1e-05
iteration: 313000 loss: 0.0023 lr: 1e-05
iteration: 314000 loss: 0.0023 lr: 1e-05
iteration: 315000 loss: 0.0023 lr: 1e-05
iteration: 316000 loss: 0.0023 lr: 1e-05
iteration: 317000 loss: 0.0023 lr: 1e-05
iteration: 318000 loss: 0.0023 lr: 1e-05
iteration: 319000 loss: 0.0024 lr: 1e-05
iteration: 320000 loss: 0.0023 lr: 1e-05
iteration: 321000 loss: 0.0023 lr: 1e-05
iteration: 322000 loss: 0.0024 lr: 1e-05
iteration: 323000 loss: 0.0023 lr: 1e-05
iteration: 324000 loss: 0.0025 lr: 1e-05
iteration: 325000 loss: 0.0023 lr: 1e-05
iteration: 326000 loss: 0.0024 lr: 1e-05
iteration: 327000 loss: 0.0024 lr: 1e-05
iteration: 328000 loss: 0.0023 lr: 1e-05
iteration: 329000 loss: 0.0023 lr: 1e-05
iteration: 330000 loss: 0.0023 lr: 1e-05
iteration: 331000 loss: 0.0023 lr: 1e-05
iteration: 332000 loss: 0.0023 lr: 1e-05
iteration: 333000 loss: 0.0023 lr: 1e-05
iteration: 334000 loss: 0.0023 lr: 1e-05
iteration: 335000 loss: 0.0023 lr: 1e-05
iteration: 336000 loss: 0.0023 lr: 1e-05
iteration: 337000 loss: 0.0023 lr: 1e-05
iteration: 338000 loss: 0.0023 lr: 1e-05
iteration: 339000 loss: 0.0023 lr: 1e-05
iteration: 340000 loss: 0.0023 lr: 1e-05
iteration: 341000 loss: 0.0023 lr: 1e-05
iteration: 342000 loss: 0.0024 lr: 1e-05
iteration: 343000 loss: 0.0025 lr: 1e-05
iteration: 344000 loss: 0.0022 lr: 1e-05
iteration: 345000 loss: 0.0023 lr: 1e-05
iteration: 346000 loss: 0.0024 lr: 1e-05
iteration: 347000 loss: 0.0022 lr: 1e-05
iteration: 348000 loss: 0.0023 lr: 1e-05
iteration: 349000 loss: 0.0023 lr: 1e-05
iteration: 350000 loss: 0.0023 lr: 1e-05
iteration: 351000 loss: 0.0024 lr: 1e-05
iteration: 352000 loss: 0.0023 lr: 1e-05
iteration: 353000 loss: 0.0023 lr: 1e-05
iteration: 354000 loss: 0.0023 lr: 1e-05
iteration: 355000 loss: 0.0022 lr: 1e-05
iteration: 356000 loss: 0.0023 lr: 1e-05
iteration: 357000 loss: 0.0023 lr: 1e-05
iteration: 358000 loss: 0.0023 lr: 1e-05
iteration: 359000 loss: 0.0022 lr: 1e-05
iteration: 360000 loss: 0.0023 lr: 1e-05
iteration: 361000 loss: 0.0022 lr: 1e-05
iteration: 362000 loss: 0.0023 lr: 1e-05
iteration: 363000 loss: 0.0023 lr: 1e-05
iteration: 364000 loss: 0.0023 lr: 1e-05
iteration: 365000 loss: 0.0023 lr: 1e-05
iteration: 366000 loss: 0.0022 lr: 1e-05
iteration: 367000 loss: 0.0022 lr: 1e-05
iteration: 368000 loss: 0.0024 lr: 1e-05
iteration: 369000 loss: 0.0023 lr: 1e-05
iteration: 370000 loss: 0.0022 lr: 1e-05
iteration: 371000 loss: 0.0022 lr: 1e-05
iteration: 372000 loss: 0.0022 lr: 1e-05
iteration: 373000 loss: 0.0022 lr: 1e-05
iteration: 374000 loss: 0.0022 lr: 1e-05
iteration: 375000 loss: 0.0022 lr: 1e-05
iteration: 376000 loss: 0.0022 lr: 1e-05
iteration: 377000 loss: 0.0022 lr: 1e-05
iteration: 378000 loss: 0.0023 lr: 1e-05
iteration: 379000 loss: 0.0023 lr: 1e-05
iteration: 380000 loss: 0.0023 lr: 1e-05
iteration: 381000 loss: 0.0022 lr: 1e-05
iteration: 382000 loss: 0.0022 lr: 1e-05
iteration: 383000 loss: 0.0022 lr: 1e-05
iteration: 384000 loss: 0.0022 lr: 1e-05
iteration: 385000 loss: 0.0022 lr: 1e-05
iteration: 386000 loss: 0.0022 lr: 1e-05
iteration: 387000 loss: 0.0023 lr: 1e-05
iteration: 388000 loss: 0.0022 lr: 1e-05
iteration: 389000 loss: 0.0024 lr: 1e-05
iteration: 390000 loss: 0.0022 lr: 1e-05
iteration: 391000 loss: 0.0022 lr: 1e-05
iteration: 392000 loss: 0.0022 lr: 1e-05
iteration: 393000 loss: 0.0022 lr: 1e-05
iteration: 394000 loss: 0.0022 lr: 1e-05
iteration: 395000 loss: 0.0022 lr: 1e-05
iteration: 396000 loss: 0.0022 lr: 1e-05
iteration: 397000 loss: 0.0022 lr: 1e-05
iteration: 398000 loss: 0.0022 lr: 1e-05
iteration: 399000 loss: 0.0022 lr: 1e-05
iteration: 400000 loss: 0.0022 lr: 1e-05
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/core/train.py", line 83, in load_and_enqueue
    sess.run(enqueue_op, feed_dict=food)
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File "/home/jonas2/miniconda/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1115, in _run
    raise RuntimeError('Attempted to use a closed Session.')
RuntimeError: Attempted to use a closed Session.
{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset88shuffle3/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'adam', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 8, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]], 'all_joints_names': ['t3L', 'wstL', 't5L', 'elbL', 'shdL', 'ankL', 'nl', 'str', 'lmb', 'shdR', 'ankR', 'elbR', 'wstR', 't5R', 't3R', 'tail'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC88shuffle3.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_88shuffle3.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]], 'net_type': 'resnet_101', 'num_joints': 16, 'pos_dist_thresh': 17, 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'gaussian_blur': False, 'gaussian_blur_params': {'sigma': [0.5, 4.0]}, 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]], 'fliplr': False, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}, 'multiply_and_add_to_brightness': False, 'snow': False, 'clouds': False, 'fog': False, 'snow_flakes': False, 'rain': False}
Starting training....
The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.
Selecting single-animal trainer
Batch Size is 8
Loading ImageNet-pretrained resnet_101
Training parameter:
{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29/data_augm_00_none/dlc-models/iteration-0/geneva_protocol_paper_austin_2020_bat_dataJul29-trainset89shuffle4/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'adam', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 8, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]], 'all_joints_names': ['t3L', 'wstL', 't5L', 'elbL', 'shdL', 'ankL', 'nl', 'str', 'lmb', 'shdR', 'ankR', 'elbR', 'wstR', 't5R', 't3R', 'tail'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/geneva_protocol_paper_austin_2020_bat_data_DLC89shuffle4.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/home/jonas2/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_geneva_protocol_paper_austin_2020_bat_dataJul29/Documentation_data-geneva_protocol_paper_austin_2020_bat_data_89shuffle4.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.0001, 7500], ['5e-05', 12000], ['1e-05', 400000]], 'net_type': 'resnet_101', 'num_joints': 16, 'pos_dist_thresh': 17, 'project_path': '/home/jonas2/DLC_files/projects/geneva_protocol_paper_austin_2020_bat_data-DLC-2022-07-29', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'gaussian_blur': False, 'gaussian_blur_params': {'sigma': [0.5, 4.0]}, 'symmetric_pairs': [[0, 14], [1, 12], [2, 13], [3, 11], [4, 9], [5, 10]], 'fliplr': False, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}, 'multiply_and_add_to_brightness': False, 'snow': False, 'clouds': False, 'fog': False, 'snow_flakes': False, 'rain': False}
Starting training....
The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.
